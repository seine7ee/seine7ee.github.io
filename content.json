{"meta":{"title":"everything`s cola","subtitle":null,"description":null,"author":"seine7ee","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-07-22T12:24:51.000Z","updated":"2019-07-22T12:25:31.460Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"TF-IDF学习","slug":"TF-IDF学习","date":"2020-02-28T08:52:52.000Z","updated":"2020-02-28T14:08:06.965Z","comments":true,"path":"2020/02/28/TF-IDF学习/","link":"","permalink":"http://yoursite.com/2020/02/28/TF-IDF学习/","excerpt":"","text":"TF-IDF学习1. 介绍TF-IDF(term frequency - inverse document frequence)是一种用于信息检索和数据挖掘的常用加权技术。使用TF-IDF是因为计算机只能识别数字，对于文本总出现的单词，计算机无法对它们进行数据处理，需要将文本转换成数学上的概念，便于进一步的训练。 思想： 字词的重要性与该字词在文件中出现的次数成正比，而与该字词在语料库中出现的次数成反比。也就是说，如果一个单词在某篇文章中出现的频率较高，而在其他文章中出现的频率不是很高，则认为该词语具有很好的区分能力，适合用来做分类。 (1) TF是指单词在一个文本中出现的频率，即某个单词的频次/文档中所有单词的频次总和： TF_{i} = {n_{i}\\over \\sum_{j=0}^kn_j}(2) IDF是指逆文档频率，即某个词语的IDF可以由语料库中的文档总数除以包含该单词的文档数，然后取对数得到，它的计算方式如下（加一是为了避免分母为0的情况）： IDF_i = log({文档总数 \\over {包含单词i的文档数 + 1}})(3) TF-IDF就是TF * IDF​ TF-IDF较为简单，没有考虑语义信息，因此不能处理一词多义和一义多词的情形。 (4) 计算过程介绍一个文本经过TF-IDF处理以后，就会变成文本向量，具体如下： 原始文本 1234&gt; text = '人来又人往，玫瑰色花香。已悄悄成为过往。'&gt; # 分词文本&gt; text = ['人 来 又 人往 ， 玫瑰色 花香 。', '已 悄悄 成为 过往 。']&gt; &gt; 获取词袋 text中有两句话，可以将每一句话都看作一个文本，我们可以获取所有文本中的单词(即获取词袋中的单词)，词袋中的单词如下(ordered)， 词袋中单词的数目为6个： 12&gt; ['人往', '悄悄', '成为', '玫瑰色', '花香', '过往']&gt; &gt; 构建文本向量 每一个文本的文本向量的维度是(1, 词袋长度)，那么对于text中的第一句话而言，它的分词结果如下（去除停用词后）： 12&gt; '人 人往 玫瑰色 花香'&gt; &gt; 那么在第一句话的文本向量中的每个元素就是，词袋中每个单词出现在第一句话中的频次。 频次 1 0 0 1 1 0 单词 人往 悄悄 成为 玫瑰色 花香 过往 计算TF-IDF值 这样就为每个文本（每句话）得到了一个向量，以第一句话为例，向量如下： | TF-IDF | 0.58 | 0. | 0. | 0.58 | 0.58 | 0. || ——— | —— | —— | —— | ——— | —— | —— || 单词 | 人往 | 悄悄 | 成为 | 玫瑰色 | 花香 | 过往 | 2. 应用在搜索引擎，关键词提取，文本相似性和文本分类中都有使用。 3. python用sklearn实现中文文本的TF-IDF计算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 使用正则表达式实现文本分句def sent_segmentor(para, chn=True): import re if chn: para = re.sub(r'([。！？\\?])([^”’])', r'\\1\\n\\2', para) para = re.sub(r'(\\.&#123;6&#125;)([^”’])', r'\\1\\n\\2', para) # 匹配出英文省略号 para = re.sub(r'(\\…&#123;2&#125;)([^”’])', r'\\1\\n\\2', para) # 匹配中文省略号，并分行 para = re.sub(r'([。！？\\?])([”’])', r'\\1\\n\\2', para) para = para.rstrip() return re.split(r'\\n', para) else: para = re.sub(r'([\\.\\!\\?])([^\\\"\\'])', r'\\1\\n\\2', para) para = re.sub(r'(\\.&#123;6&#125;)([^\\\"\\'])', r'\\1\\n\\2', para) para = re.sub(r'([\\.\\!\\?])([\\\"\\'])', r'\\1\\n\\2', para) para = para.rstrip() return para.split(r'\\n') # 导入停用词表def stopwords(url): with open(url, 'r', encoding='utf-8') as file: stopwords = file.readlines() stopwords = [line.strip() for line in stopwords] return stopwords# 导入sklearn需要的包from sklearn import feature_extractionfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerif __name__ == \"__main__\": # 导入pkuseg包，用于后面分词 import pkuseg text = '人来又人往，玫瑰色花香。已悄悄成为过往。' stopwords = stopwords(r\"D:\\nlp_tools\\stopwords\\HIT_stopwords.txt\") # 加载分词模型 seg = pkuseg.pkuseg() # 构建语料库 sents = [] for line in sent_segmentor(text): words = [] for word in seg.cut(line): if word not in stopwords: words.append(word) words = \" \".join(words) sents.append(words.strip()) vectorizer = CountVectorizer() tf_idf = TfidfTransformer() matrix = vectorizer.fit_transform(sents) print(\"这是词袋：\") print(vectorizer.get_feature_names()) print(\"这是词频矩阵：\") print(matrix.toarray()) word_tf_idf = tf_idf.fit_transform(matrix) print(\"这是tf-idf矩阵：\") print(word_tf_idf.toarray()) 输出： 12345678910这是词袋：['人往', '悄悄', '成为', '玫瑰色', '花香', '过往']这是词频矩阵：[[1 0 0 1 1 0] [0 1 1 0 0 1]]这是tf-idf矩阵：[[0.57735027 0. 0. 0.57735027 0.57735027 0. ] [0. 0.57735027 0.57735027 0. 0. 0.57735027]] 4. 不足TF-IDF 采用文本逆频率 IDF 对 TF 值加权取权值大的作为关键词，但 IDF 的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以 TF-IDF 算法的精度并不是很高，尤其是当文本集已经分类的情况下。 在本质上 IDF 是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF 的简单结构并不能使提取的关键词， 十分有效地反映单词的重要程度和特征词的分布情 况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被盖。 TF-IDF算法实现简单快速，但是仍有许多不足之处：（1）没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 （2）按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 （3）传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。 （4）对于文档中出现次数较少的重要人名、地名信息提取效果不佳。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[]},{"title":"Keyword Extraction by TextRank","slug":"Keyword-Extraction-by-TextRank","date":"2019-08-01T02:58:12.000Z","updated":"2019-11-22T08:03:28.852Z","comments":true,"path":"2019/08/01/Keyword-Extraction-by-TextRank/","link":"","permalink":"http://yoursite.com/2019/08/01/Keyword-Extraction-by-TextRank/","excerpt":"","text":"Keyword Extraction by TextRank1. Understand PageRankTextRank是一种基于PageRank的算法，通常用来进行关键词抽取和文本摘要生成。PageRank是一个用来计算网页权值的算法，我们可以把网络上的所有网页认为是一个巨大的有向图，一些网页指向另外一些网页，而在图中，一个结点就对应一个网页。如果网页A有通向网页B的连接，则这个连接就可以被认作一个从A到B的有向边。 当我们构建完一整张图以后，我么可以为网页分配权值，使用以下公式： S(V_i) = (1-d) + d * \\sum_{j\\in In(V_i)} { {1}\\over{|Out(V_j)|} }S(V_j) $S(V_i)$ - the weight of webpage i $d$ - damping factor, in case of no outgoing links $In(V_i)$ - inbound links of i, which is a set $Out(V_j)$ - outgoing links of j, which is a set $|Out(V_j)|$ - the number of outbound links 下面用一个例子来讲述PageRank的实现过程。 我们可以用上面的有向图来表示网页之间的连接关系，下图中表示A有一个结点指向E，B有两个结点指向E，我们可以使用邻接矩阵来表示这个有向图。 每一行中的顶点对应的值表示从其他顶点连入该顶点的连接数，即该顶点的入度，同样地，每一列中的顶点对应的值也即该顶点的出度——即该顶点连接出去的数目。 A B E F A 0 0 0 0 B 0 0 0 0 E 1 1 0 0 F 0 1 0 0 根据公式${1 \\over {Out(V_j)}}$，我们应对上述的矩阵作标准化处理，这一步对矩阵所做的处理，实际上也就是将矩阵转换成马尔可夫的转移概率矩阵，通过Markov Chains(实际上网页之间的连接就是一个Markov Chains)的性质来得出最终收敛结果(即该Markov Chains的平稳分布)，也就是我们需要的PageRank。 A B E F A 0 0 0 0 B 0 0 0 0 E 1 0.5 0 0 F 0 0.5 0 0 然后，我们令所有节点的默认权值为1，完成$\\sum_{j\\in In(V_i){1\\over|Out(V_j)|}S(V_j)}$ 的运算，具体如下： \\left[ \\begin{matrix} 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0\\\\ 1 & 0.5 & 0 & 0\\\\ 0 & 0.5 & 0 & 0 \\end{matrix} \\right]* \\left[ \\begin{matrix} 1\\\\ 1\\\\ 1\\\\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} 0\\\\ 0\\\\ 1.5\\\\ 0.5 \\end{matrix} \\right]最后，经过多次迭代直到收敛，就可以得出最后的权值。因为我们预先并不知道node的最终权值，因此，具体的收敛实现则可以用相近两次的权值总和(sum(pagerank))之差来表示，如果这个差小于0.00001，就认为收敛了，即权值就是最终权值。 2.使用TextRank实现Keyword ExtractionPageRank和TextRank的基本思想是一致的，在PageRank中，node是网页，而在TextRank中，node则是单词。在关键词抽取中，我们可以把一个文本分割成许多句子，然后我们存储这些句子中特定词性的词，因为句子中的大多数单词对决定句子的重要性并不是那么有用，我们只考虑名词(NOUN)，专有名词(PROPN)，动词(VERB)等词性，当然这是可以选择的，你也可以使用所有的单词。 例如，我们使用这样一段文本， 12345678910111213141516171819202122232425# e.g.1 将整段文本分割成几个句子 import spacy# 导入en_core_web_sm模型nlp = spacy.load('en_core_web_sm')# 实验文本content = '''The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.'''# 将文本转换为一个nlp对象doc = nlp(content)# 打印文本中的句子for sents in doc.sents: print(sents.text) [out]:# 实验文本被分成了三句话The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics.At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking.While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse. 如果要进行关键词的抽取，那么主要包括以下几个内容： 文本分割成句子，保存每一句中特定词性的单词(表征词 - tokens) 以每一句为单位，获得该句子中指定窗口大小的pair，最终得到整个文本的token_pairs集合 获得所有的tokens的集合，即该文本的表征词汇集合 - vocab 根据获得的vocab和token_pairs，来得到文本图的矩阵表示，并将矩阵归一化 迭代计算出每个token的weight (可选操作)向STOP_WORDS中加入自定义的STOP_WORDS 对权值进行排序 pre-option12345678910import spacyfrom spacy.lang.en.stop_words import STOP_WORDSimport numpy as npfrom collections import OrderedDictnlp = spacy.load(\"en_core_web_sm\")content = '''The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.'''doc = nlp(content) 1. 文本分割成句子，保存每一句中特定词性的单词1234567891011121314151617181920# sentence_segmentcandidate_pos = ['NOUN', 'PROPN']sentences = []for sent in doc.sents: selected_word = [] for word in sent: if word.pos_ in candidate_pos and word.is_stop is False: selected_word.append(word) sentences.append(selected_word)for sent in sentences: print(sent, \"\\n\") [out][Wandering, Earth, China, ’s, budget, science, fiction, thriller, screens, AMC, theaters, North, America, weekend, filmmaking, —, spectacles, China, ’s, epics] [time, Wandering, Earth, throwback, eras, filmmaking] [film, ’s, cast, tone, Chinese, science, fiction, fans, lot, screen, movies] 2. 以每一句为单位，获得该句子中指定窗口大小的pair，最终得到整个文本的token_pairs集合12345678910111213# get_token_pairstoken_pairs = []window_size = 4for sent in sentences: for i, word in enumerate(sent): for j in (i+1, i+window_size): if j &gt;= len(sent): break pair = (word, sent[j]) if pair not in token_pairs: token_pairs.append(pair)print(token_pairs) ​ [output] 1[(Wandering, Earth), (Wandering, budget), (Earth, China), (Earth, science), (China, ’s), (China, fiction), (’s, budget), (’s, thriller), (budget, science), (budget, screens), (science, fiction), (science, AMC), (fiction, thriller), (fiction, theaters), (thriller, screens), (thriller, North), (screens, AMC), (screens, America), (AMC, theaters), (AMC, weekend), (theaters, North), (theaters, filmmaking), (North, America), (North, —), (America, weekend), (America, spectacles), (weekend, filmmaking), (weekend, China), (filmmaking, —), (filmmaking, ’s), (—, spectacles), (—, epics), (spectacles, China), (China, ’s), (’s, epics), (time, Wandering), (time, eras), (Wandering, Earth), (Wandering, filmmaking), (Earth, throwback), (throwback, eras), (eras, filmmaking), (film, ’s), (film, Chinese), (’s, cast), (’s, science), (cast, tone), (cast, fiction), (tone, Chinese), (tone, fans), (Chinese, science), (Chinese, lot), (science, fiction), (science, screen), (fiction, fans), (fiction, movies), (fans, lot), (lot, screen), (screen, movies)] 3. 获得所有的tokens的集合，即该文本的表征词汇集合 - vocab1234567891011# get_vocabvocab = OrderedDict()i = 0for sent in sentences: for word in sent: if word not in vocab: vocab[word] = i i += 1print(vocab)len(vocab) ​ [output] 1OrderedDict([(Wandering, 0), (Earth, 1), (China, 2), (’s, 3), (budget, 4), (science, 5), (fiction, 6), (thriller, 7), (screens, 8), (AMC, 9), (theaters, 10), (North, 11), (America, 12), (weekend, 13), (filmmaking, 14), (—, 15), (spectacles, 16), (China, 17), (’s, 18), (epics, 19), (time, 20), (Wandering, 21), (Earth, 22), (throwback, 23), (eras, 24), (filmmaking, 25), (film, 26), (’s, 27), (cast, 28), (tone, 29), (Chinese, 30), (science, 31), (fiction, 32), (fans, 33), (lot, 34), (screen, 35), (movies, 36)]) 4. 根据获得的vocab和token_pairs，来得到文本图的矩阵表示，并将矩阵归一化12345678910111213141516# get_matrixvocab_size = len(vocab)matrix_for_text = np.zeros((vocab_size, vocab_size), dtype='float')for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] matrix_for_text[i][j] = 1# 在关键词抽取中，我们把文本看做了一个无向图，无向图的矩阵表示是一个对称矩阵，以下就是无向图矩阵的对称化matrix_for_text = matrix_for_text + matrix_for_text.T - np.diag(matrix_for_text.diagonal())# 对矩阵的每一列进行求和 -- 也就是该列顶点对应的出度总和norm = np.sum(matrix_for_text, axis=0)# normalization以后的矩阵matrix_for_text = np.divide(matrix_for_text, norm, where=(norm!=0)) 5. 迭代计算出每个token的weight1234567891011121314151617# analyze()min_diff = 1e-5d = 0.85iterations = 10tr = np.array([1] * vocab_size)print(tr)previous_tr = 0for i in range(iterations): tr = 1-d + d*(np.dot(matrix_for_text, tr)) if abs(previous_tr - sum(tr)) &lt; min_diff: break else: previous_tr = sum(tr)print(tr) ​ [output] 1234567[0.72611111 0.91529514 0.92414931 0.91352431 1.10772569 1.16616319 1.08086806 1.04574653 1.0903125 1.03010417 1.03010417 1.0903125 1.04574653 1.08086806 1.16616319 1.10772569 0.91352431 0.92414931 0.91529514 0.72611111 0.9575 1.12395833 0.91854167 0.91854167 1.12395833 0.9575 0.74116319 0.93034722 1.00442708 0.96428819 1.19361111 1.33232639 1.19361111 0.96428819 1.00442708 0.93034722 0.74116319] 6. 对权值进行排序123456789# get_keywords()node_weight = dict()for word, index in vocab.items(): node_weight[word] = tr[index]node_weight_ordered = OrderedDict(sorted(node_weight.items(), key=lambda t:t[1], reverse=True))for k, v in node_weight_ordered.items(): print(k, \" -- \", v) ​ [output] 12345678910111213141516171819202122232425262728293031323334353637science -- 1.3323263888888888Chinese -- 1.1936111111111112fiction -- 1.1936111111111112science -- 1.1661631944444442filmmaking -- 1.1661631944444442Wandering -- 1.1239583333333334eras -- 1.1239583333333334budget -- 1.1077256944444445— -- 1.1077256944444445screens -- 1.0903125North -- 1.0903125fiction -- 1.0808680555555554weekend -- 1.0808680555555554thriller -- 1.0457465277777778America -- 1.0457465277777778AMC -- 1.0301041666666666theaters -- 1.0301041666666666cast -- 1.004427083333333lot -- 1.004427083333333tone -- 0.9642881944444444fans -- 0.9642881944444444time -- 0.9574999999999999filmmaking -- 0.9574999999999999screen -- 0.9303472222222222’s -- 0.9303472222222221China -- 0.9241493055555555China -- 0.9241493055555555Earth -- 0.9185416666666666throwback -- 0.9185416666666666Earth -- 0.9152951388888888’s -- 0.9152951388888888’s -- 0.9135243055555555spectacles -- 0.9135243055555555film -- 0.7411631944444443movies -- 0.7411631944444443Wandering -- 0.726111111111111epics -- 0.726111111111111 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import numpy as npimport spacyfrom collections import OrderedDictfrom spacy.lang.en.stop_words import STOP_WORDSnlp = spacy.load(\"en_core_web_sm\")class KeywordExtraction(): def __init__(self): self.d = 0.85 # damping factor self.min_diff = 1e-5 # self.steps = 10 # iterate steps self.node_weight = None # node_weight def set_stopwords(self, stopwords): ''' 添加自定义 stop_words :param stopwords: :return: ''' for word in STOP_WORDS.union(set(stopwords)): lexeme = nlp.vocab[word] lexeme.is_stop = True def sentence_segment(self, doc, candidate_pos, lower=False): ''' 保存每一句中特定词性的单词 :return: ''' sentences = [] for sent in doc.sents: selected_words = [] for word in sent: if word.pos_ in candidate_pos and word.is_stop is False: if lower: selected_words.append(word.text.lower()) else: selected_words.append(word.text) sentences.append(selected_words) return sentences def get_vocab(self, sentences): ''' 得到所有句子中特定词性的单词 :param sentences: :return: ''' vocab = OrderedDict() i = 0 for sent in sentences: for word in sent: if word not in vocab: vocab[word] = i i += 1 return vocab def get_token_pairs(self, window_size, sentences): ''' :param window_size: :param sentences: :return: ''' token_pairs = [] for sent in sentences: for i, word in enumerate(sent): for j in range(i+1, i+window_size): if j &gt;= len(sent): break pair = (word, sent[j]) if pair not in token_pairs: token_pairs.append(pair) return token_pairs def symmetrize(self, matrix): return matrix + matrix.T - np.diag(matrix.diagonal()) def get_matrix(self, vocab, token_pairs): vocab_size = len(vocab) g = np.zeros((vocab_size, vocab_size), dtype='float') for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] g[i][j] = 1 g = self.symmetrize(g) sum = np.sum(g, axis=0) ''' np.divide(x1, x2, where=True) x1: array_like 被除矩阵 x2: array_like 除数矩阵 如果x1和x2的shape不一致的话，他们必须符合数组广播规则，也就是输出矩阵的shape -- 换言之，输入数组必须具有相同的形状或符合数组广播规则 ''' g_after_divide = np.divide(g, sum, where=sum!=0) return g_after_divide def get_keywords(self, num=10): node_weight = OrderedDict(sorted(self.node_weight.items(), key= lambda t: t[1], reverse=True)) for i, (k, v) in enumerate(node_weight.items()): print(k, \" -- \", v) if i &gt; num: break def analyze(self, doc, window_size=4, candidate_pos=['NOUN', 'PROPN'], lower=False, stopwords=list()): self.set_stopwords(stopwords) doc_sentences = self.sentence_segment(doc, candidate_pos, lower) doc_vocab = self.get_vocab(doc_sentences) doc_token_pairs = self.get_token_pairs(window_size, doc_sentences) doc_matrix = self.get_matrix(doc_vocab, doc_token_pairs) vocab_size = len(doc_vocab) tr = np.array([1] * vocab_size) previous_tr = 0 for i in range(self.steps): tr = 1-self.d + self.d * np.dot(doc_matrix, tr) if abs(previous_tr - sum(tr)) &lt; self.min_diff: break previous_tr = sum(tr) node_weight = dict() for word, index in doc_vocab.items(): node_weight[word] = tr[index] self.node_weight = node_weightif __name__ == '__main__': textrank = KeywordExtraction() text = ''' The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse. ''' doc = nlp(text) textrank.analyze(doc, 4) # 取前10个权值最高的单词 textrank.get_keywords(10)","categories":[{"name":"TextRank","slug":"TextRank","permalink":"http://yoursite.com/categories/TextRank/"}],"tags":[]},{"title":"Python正则表达式-不完全使用指南","slug":"Python正则表达式-不完全使用指南","date":"2019-07-22T12:39:52.000Z","updated":"2019-08-01T03:23:01.027Z","comments":true,"path":"2019/07/22/Python正则表达式-不完全使用指南/","link":"","permalink":"http://yoursite.com/2019/07/22/Python正则表达式-不完全使用指南/","excerpt":"","text":"1. 基础字符1.1 元字符元字符是一些特殊的metacharactes， 并且不匹配自己。相反，他们表示应该匹配一些与众不同的东西，或者通过重复他们或改变他们的含义来影响正则的其他部分。元字符如下： . ^ $ * + ? { } [ ] \\ | ( ) 在元字符”[ ]”中，可以单独列出字符，也可以通过给出两个字符并用”-“标记将它们分开来表示一系列字符。例如，[abc]将匹配任何字符a、b或c（匹配单个字符），这与[a-c]相同，它使用一个范围来表示同一组字符。 字符类中的元字符不生效。例如，[akm$]将匹配’a’, ‘k’, ‘m’, ‘$’中的任意字符; ‘$’通常是一个元字符，但在一个字符类中它被剥夺了特殊性。 1.2 预定义字符集 \\d 匹配任何十进制数字；这等价于[0-9] \\D 匹配任何非数字字符；这等价于[^0-9] \\s 匹配任何空白字符；这等价于[ \\t\\n\\r\\f\\v] \\S 匹配任何非空白字符；这等价于[^ \\t\\n\\r\\f\\v] \\w 匹配任何字母与数字字符；这相当于类[a-zA-Z0-9] \\W 匹配任何非字母与数字字符；这相当于类[^a-zA-Z0-9] 1.3 常用操作符 操作符 说明 实例 . 表示任意单个字符 对于大小写字母、各种符号都能匹配 [ ] 字符集，对单个字符给出取值范围 [abc]表示a、b、c,[a-z]表示a到z的单个字符 非字符集，对单个字符给出排除范围 abc 表示非a或b或c的单个字符 * 前一个字符0次或无限次扩展 abc*表示ab、abc、abcc、abccc等 + 前一个字符1次或无限次扩展 abc+表示abc、abcc、abccc等 ？ 前一个字符0次或1次扩展 abc？表示ab、abc \\ 左右表达式任意一个 abc\\ def表示abc、def {m} 扩展前一个字符m次 ab{2}c表示abbc {m, n} 扩展前一个字符m至n次（包 含n） ab{1, 2}表示ab、abb ^ 匹配字符串开头 ^abc表示abc且在一个字符串的开头 $ 匹配字符串结尾 abc$表示abc且在一个字符串的结尾 ( ) 分组标记，内部只能使用 \\ 操作符 (abc)表示abc，(abc \\ def)表示abc、def \\d 表示十进制数字 \\w 匹配任何字母与数字字符 相当于[a-zA-Z0-9] tips: ?字符可以看作是一个可选字符，如home-?grew，其表示home-grew或者homegrew，即当作 ? 的前一个字符是可选的的； 对于{m, n}来说，如果不限定m和n的数值，即{ , }，则认为下限m的值为0，而上限n的值则为无穷，因此： *也就等价于{0, } +也就等价于{1, } ?也就等价于{0, 1} 但在实际中，还是建议使用*、+、?的形式，这样更短更容易阅读 2. 使用正则表达式有了前面的基础字符知识，我们已经可以使用正则表达式了，在python中如何使用它们呢？re模块提供了正则表达式引擎的接口，允许你将正则编译为对象，然后用它们进行匹配。 2.1 编译正则表达式正则表达式首先要编译成模式对象，然后才能进行各种操作。 12345678import rere_pattern = re.compile('ab*')print(\"re_pattern : \",re_pattern)print(\"type :\", type(re_pattern))[output]re_pattern : re.compile('ab*')type : &lt;class 're.Pattern'&gt; 需要明确的是，正则是作为字符串传递给re.compile()的，正则被处理为字符串，因为正则表达式不是核心python语言的一部分，并且没有创建用于表达它们的特殊语法。 可以理解的是，在正则被re模块编译之前，它就是一个普通的字符串，这与python语言所理解的字符串并无二致，如前所述，正则表达式使用反斜杠字符 \\ 来表示特殊形式或允许使用特殊字符而不调用它们的特殊含义，这就会与python字符串中的反斜杠用途发生冲突，导致反斜杠灾难。 例如我们要编写一个与 \\expression相匹配的正则，那么编译之后得到的目标模式就应该是 \\ \\expression，在编译之前，正则就是一个普通的字符串，对于每一个 \\，在字符串中表示就是 \\ \\，也就是说，编译前的字符串就应该是 \\ \\ \\ \\ expression，可见，对目标字符串中如果有一个反斜杠，那么传给编译器的正则表达式中就会有4个反斜杠，十分繁琐。 字符 阶段 \\expression 被匹配的字符串 \\ \\ expression 为 re.compile()转义的反斜杠 “\\\\\\\\expression” 为字符串字面转义的反斜杠 解决方案是使用python的原始字符串表示法来表示正则表达式；反斜杠不以任何特殊的方式处理前缀为 ‘r’的字符串字面，因此 r’\\n’ 是一个包含 ‘\\’和 ‘n’的双字符字符串，而 ‘\\n’是一个包含换行符的单字符字符串。正则表达式通常使用这种原始字符串表示法用python代码编写。 常规字符串 原始字符串 “ab*” r”ab*” “\\\\\\\\expression” r”\\\\expression” “\\\\w+\\\\s+\\\\l” r”\\w+\\s+\\l” tips: 1234567891011121314&gt; # 带反斜杠的特殊字符可以用\\d或者\\\\d来匹配&gt; pattern = re.compile(\"\\\\d\")&gt; result = pattern.findall(\"space hh99\")&gt; print(result)&gt; &gt; [output] : ['9', '9']&gt; &gt; # 匹配一个反斜杠&gt; pattern = re.compile(\"\\\\\\\\\")&gt; result = pattern.findall(\"n\\\\b\\\\a\")&gt; print(result)&gt; &gt; [output] : ['\\\\', '\\\\']&gt; &gt; 3. 应用匹配3.1 re库的常用函数 函数 说明 search() 扫描字符串，查找此正则匹配的任何位置，返回match对象 match() 从一个字符串的开始位置其匹配正则表达式，返回match对象 findall() 搜索字符串，以列表类型返回全部能匹配的子串 split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型 finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 3.2 匹配对象 方法/属性 目的 group() 返回正则匹配的字符串 start() 返回匹配的开始位置 end() 返回匹配的结束位置 span() 返回包含匹配(start, end)位置的元组 group()返回正则匹配的子字符串。start()和end()返回匹配的其实和结束索引。span()在单个元组中返回开始和结束索引。由于match()方法只检查正则是否在字符串的开头匹配，所以start()将始终为0。 注意： 只有match对象才有上述属性，也就是说，对一个模式只有使用search()、match()和finditer()后所返回的match对象才有上述属性，而对于使用findall()等方法，由于其返回对象是一个list，所以没有上述属性。 1234567891011121314151617181920212223242526import reresult1 = pattern.match(\"little boy found his favorite toy\")result2 = pattern.match(\"littleboyfoundhisfavoritetoy\")print(result1)print(result2)[output]&gt;&gt;&gt; &lt;re.Match object; span=(0, 6), match='little'&gt;&gt;&gt;&gt; &lt;re.Match object; span=(0, 28), match='littleboyfoundhisfavoritetoy'&gt;result2.group()[output]&gt;&gt;&gt; 'littleboyfoundhisfavoritetoy'result2.start()[output]&gt;&gt;&gt; 0result2.end()[output]&gt;&gt;&gt; 28result2.span()[output]&gt;&gt;&gt; (0, 28) 但是，模式的search()方法会扫描字符串，因此在这种情况下匹配可能不会从0开始。 12345678910111213141516result3 = pattern.search(\"****little boy`s here\")result3.group()[output]&gt;&gt;&gt; 'little'result3.start()[output]&gt;&gt;&gt; 4result3.end()[output]&gt;&gt;&gt; 10result3.span()[output]&gt;&gt;&gt; (4, 10) 4. 更多模式能力4.1 分组通常，你需要获取更多信息，而不仅仅是正则是否匹配。正则表达式通常用于通过将正则分成几个子组来解析字符串，这些子组匹配不同的感兴趣组件。 例如，现有如下信息： From: author@example.com User-Agent: Thunderbird MIME-Version: 1.0 To: editor@example 我们想要提取每一个 ：后面的信息，因此可以使用分组来进行匹配， 1234567891011121314151617181920212223242526272829303132# e.g. 4.1.1info = \"From: author@example.com\\nUser-Agent: Thunderbird\\nMIME-Version: 1.0\\nTo: editor@example\"print(info)[output]&gt;&gt;&gt; From: author@example.com User-Agent: Thunderbird MIME-Version: 1.0 To: editor@examplepattern1 = re.compile(r\"From:\\s+(\\w+.\\w+.\\w+)\\s+User-Agent:\\s+(\\w+)\\s+MIME-Version:\\s+(\\d+.\\d+)\\s+To:\\s+(\\w+.\\w+)\", re.IGNORECASE)result = pattern1.findall(info)print(result)[output]&gt;&gt;&gt;[('author@example.com', 'Thunderbird', '1.0', 'editor@example')]result1 = pattern1.finditer(info)for result in result1: result.group(1) result.group(2) result.group(3) result.group(4) Out[108]: 'author@example.com'Out[108]: 'Thunderbird'Out[108]: '1.0'Out[108]: 'editor@example' 再上述4.1.1的例子中，我们在模式中写了四个分组，分别匹配From、User-Agent、MIME-Version和To后面的内容，这四个分组被自动地编号，分别为1、2、3、4，同时被捕获到了相应的内存中，这就引入了分组的后向引用，例如，我要进行一个双词的匹配，请看下例： 123456789101112#e.g. 4.1.2raw = \"loving loving can hurt\"'''在匹配之前，(\\w+)这个分组可以匹配任何单词内容，一旦当这个分组匹配完毕，在这里，一开始就匹配了loving这个单词，因此，就把loving捕获，放在分组1的内存单元，再经过一个空格后，把分组1中的捕获内容取出，与当前单词比较，看是否匹配，如果匹配，则结束，否则，重新匹配这个分组。'''pattern = re.compile(r\"\\b(\\w+)\\b\\s+\\1\")print(pattern.findall(raw))[output]&gt;&gt;&gt;['loving'] 4.2 忽略某个分组有时候给正则的某个子表达式加括号并不是为了分组，而仅仅是为了看起来更清晰，因此我们并不需要捕获该分组，那么，可以使用(?:expression)来忽略该分组。 123456789101112131415161718192021# e.g. 4.2.1raw = \"age:13, name:Tom\"pattern = re.compile(r\"age.(\\d+).\\s*name.(\\w+)\")print(pattern.findall(raw))[out]: [('13', 'Tom')]print(pattern.search(raw).group(1))[out]: 13 print(pattern.search(raw).group(2))[out]: Tom pattern1 = re.compile(r\"age.(\\d+).\\s*name.(?:\\w+)\")print(pattern1.findall(raw))[out]: ['13']print(pattern1.search(raw).group(1))[out]: 13# 由于忽略了第二个分组，因此无法检索该分组print(pattern1.search(raw).group(2))[out]: IndexError: no such group 需要注意的是，除了你无法检索组匹配内容的事实外，非捕获组的行为与捕获组完全相同；你可以在里面放任何东西，用重复元字符重复它，比如*，燃火把它嵌入其他组（捕获或不捕获）。(?:…..)在修改现有模式的时候特别有用，因为你可以添加新组而不更改所有其他组的编号方式。红色部分不懂什么意思。值得一提的是，捕获和非捕获组之间的搜索没有性能差异，两种形式没有哪一种更快。 4.3 命名分组命名组的语法是python特定的扩展之一：(?P…)。name是该分组的名称。命名组的行为与捕获组完全相同，并且还将名称与组关联。处理捕获组的的匹配对象方法都接受 1. 按编号引用该分组；2. 按组名字符串引用该分组。 12345678910raw = \"sheeran sheeran is good\"pattern = re.compile(r\"(?P&lt;name&gt;\\b\\w+\\b)\\s+(?P=name)\")print(pattern.findall(raw))[out]: ['sheeran']pattern = re.compile(r\"(?P&lt;name&gt;\\b\\w+\\b)\\s+\\1\")print(pattern.findall(raw))[out]: ['sheeran'] 4.4 前向断言另一个零宽度断言是前向断言。前向断言以正面和负面形式提供，如下所示： 正前向断言 — (?=…)如果包含的正则表达式，由...表示，在当前位置成功匹配，则成功，否则失败。但是，一旦尝试了包含的表达式，匹配的引擎就不会前进，模式其余的部分会在断言开始的地方尝试。 12345678910# 选出得分大于30分的球员raw = [\"James:36.33\", \"Bryant:33.2\", \"ONeal:24.33\"]pattern = re.compile(r\".+(?=[:][3][0-9][.][0-9]+)\")for it in raw: print(pattern.findall(it)) [out]: ['James'] ['Bryant'] [] 负前向断言 — (?!…)如果包含的表达式在字符串中的当前位置不匹配，则成功。 123456789101112131415161718192021'''现有一些文件如下：[\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\", \"start.end.bat\"]我们需要从中选取出后缀不是bat的文件，因此可以使用负前向断言来匹配后缀不是bat的文件'''raw = [\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\", \"start.end.bat\"]pattern = re.compile(r\".*[.](?!bat$)[^.]*\")for it in raw: print(pattern.findall(it)) [out]: ['sample.txt'] ['ss.batch'] [] ['computer.sss'] ['name.is.wiki'] ['haha.bat.ten'] [] 在上述代码中，模式是r&quot;.*[.](?!bat$)[^.]*&quot;，意思是，从结尾匹配bat，如果匹配，则该模式失败，如果结尾不匹配bat，那么开始匹配模式[^.]*$。也就是说，如果结尾不匹配bat，我们使用[^.]*$ 就可以把不匹配的后缀部分输出。如果我们使用的模式是r&quot;.*[.](?!bat$)，那么就相当于把后缀不是bat的.*[.] 输出，即只是输出后缀前面的部分: 123456789101112raw = [\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\"]pattern = re.compile(r\".*[.](?!bat$)\")for it in raw: print(pattern.findall(it)) [out]: ['sample.'] ['ss.'] [] ['computer.'] ['name.is.'] ['haha.bat.'] tips: 在平常使用中，更感觉前向断言像是一个筛选条件，我们可以匹配一个字符或字符串，在匹配对象的后面或者前面我们设定一些条件，可以用前向断言来实现。 5. 修改字符串到目前为止，我们只是针对静态字符串执行搜索。正则表达式通常也用于以各种方式修改字符串，使用以下模式方法： 方法/属性 目的 split() 将字符串拆分成一个列表，在正则匹配的任何地方将其拆分 sub() 找到正则匹配的所有子字符串，并用不同的字符串替换它们 subn() 与sub()相同，但返回新字符串和替换次数 5.1 分割字符串模式的split()方法在正则匹配的任何地方拆分字符串，返回一个片段列表。它有两种使用方式： pattern.split(string [, maxsplit = 0]) re.split(pattern, string [, maxsplit = 0]) 通过正则表达式的匹配拆分字符串，如果在正则中使用捕获括号，则它们的内容也将作为结果列表的一部分返回。如果maxsplit非零，则最多执行maxsplit次拆分。 12345678910111213# 分隔符是任意非字母数字字符序列p = re.compile(r\"\\W+\")# 不设定maxsplitresult1 = p.split(\"this is a test, short and sweet, of split()\")print(result1)[out]: ['this', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']# 设定maxsplit = 3，则最多在字符串上分隔3次result2 = p.split(\"this is a test, short and sweet, of split()\", maxsplit = 3)print(result2)[out]: ['this', 'is', 'a', 'test, short and sweet, of split()'] 如果我们不单单想要根据分隔符分隔出的文本，而且还需要知道分隔符是什么，因此，如果在正则中使用捕获括号，则分隔符的内容也会作为列表的一部分返回。 12# 在这一句中，分隔符是“， ”re.split(r\"([\\W]+)\", \"words, words, words\") 5.2 搜索和替换另一个常见任务是找到模式的所有匹配项，并用不同的字符串替换它们。sub()方法接受一个替换值，可以是字符串或函数，也可以是要处理的字符串。 pattern.sub(replacement, string[, count = 0]) 使用replacement来替换string中的匹配对象，如果没有找到模式，则string将保持不变。 可选参数count是要替换的模式最大出现次数，count必须是非负整数。默认为0表示替换所有。 123456789101112131415raw = \"sheeran is a good singer, and I think sheeran is awesome!\"pattern = re.compile(r\"sheeran\")pattern.sub(\"jay\", raw)[out]: 'jay is a good singer, and I think jay is awesome!'pattern.sub(\"Jay\", raw, count = 1)[out]: 'Jay is a good singer, and I think sheeran is awesome!'# subn的用法和sub一致，但返回一个包含新字符串值和一致性的替换次数的2元组。 pattern.subn(\"Jay\", raw)[out]: ('Jay is a good singer, and I think Jay is awesome!', 2) 需要注意的是，如果replacement是一个字符串，则处理其中的任何反斜杠转义。也就是说，\\n被转换为单个换行符，\\r 被转换为回车符，依此类推。 还有一种语法用于引用由(?P&lt;name&gt;...)语法定义的命名组。\\g&lt;name&gt; 将使用名称为name 的组匹配的子字符串，\\g&lt;number&gt; 使用相应的组号(后向引用)，因此\\g&lt;2&gt; 等同于\\2 ，但在诸如\\g&lt;2&gt;0 之类的替换字符串中并不模糊。而\\20 则被解释为对组20捕获的内容惊醒引用，而不是对组2的引用，因此不建议使用\\20 这种用法，以下几种用法都是等效的： 123456789101112131415player = \"James&#123;Forward&#125; Rondo&#123;Guard&#125; Cousins&#123;Center&#125;\"p = re.compile(r\"\\w+&#123;(?P&lt;position&gt;[^&#125;]*)&#125;\")# 把所有匹配正则的子字符串替换成 player(\\g&lt;position&gt;)p.sub(r\"player(\\g&lt;position&gt;)\", player)[out]: 'player(Forward) player(Guard) player(Center)'p.sub(r\"player(\\g&lt;1&gt;)\",player)[out]: 'player(Forward) player(Guard) player(Center)'p.sub(r\"player(\\1)\", player)[out]: 'player(Forward) player(Guard) player(Center)' 此外，replacement也可以是一个函数，它可以为你提供更多控制。如果replacement是一个函数，则为patter的每次非重叠出现将调用该函数。在每次调用时，函数都会传递一个匹配的匹配对象参数，并可以使用此信息计算所需的替换字符串并将其返回。 1234567891011121314# 实现将句中的正数转换为浮点数def transfer(match): value = int(match.group()) after_transfer = float(value) # 字符类型转换 return str(after_transfer)raw = \"James scored 33 points and Davis scored 29 points\"p = re.compile(r\"\\d+\")'''需要注意的时，sub(replacement, string)，replacement一般都是一个字符型对象。'''p.sub(transfer, raw)","categories":[{"name":"Python基础","slug":"Python基础","permalink":"http://yoursite.com/categories/Python基础/"}],"tags":[]},{"title":"leetcode-0019-删除链表的倒数第N个个节点","slug":"leetcode-0019-删除链表的倒数第N个个节点","date":"2019-06-04T06:32:20.000Z","updated":"2019-07-22T12:30:01.334Z","comments":true,"path":"2019/06/04/leetcode-0019-删除链表的倒数第N个个节点/","link":"","permalink":"http://yoursite.com/2019/06/04/leetcode-0019-删除链表的倒数第N个个节点/","excerpt":"","text":"[题目表述]给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。 示例： 给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5.说明： 给定的 n 保证是有效的。 进阶： 你能尝试使用一趟扫描实现吗？ def removeNthFromEnd_one_iter(self, head, n): dummy = ListNode(0) dummy.next = head first = dummy second = dummy while n &gt;= 0: first = first.next n -= 1 while first: second = second.next first = first.next second.next = second.next.next return dummy.next","categories":[{"name":"leetcode刷题","slug":"leetcode刷题","permalink":"http://yoursite.com/categories/leetcode刷题/"}],"tags":[]}]}