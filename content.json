{"meta":{"title":"everything`s cola","subtitle":null,"description":null,"author":"seine7ee","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-07-22T12:24:51.000Z","updated":"2019-07-22T12:25:31.460Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"[Python]基于进程的并行","slug":"Python-基于进程的并行","date":"2020-04-22T13:28:30.000Z","updated":"2020-04-22T13:53:08.269Z","comments":true,"path":"2020/04/22/Python-基于进程的并行/","link":"","permalink":"http://yoursite.com/2020/04/22/Python-基于进程的并行/","excerpt":"","text":"[Python]基于进程的并行1 subprocess - 生成多余进程 subprocess目的是开始与其他进程交互，即用来生成子进程，并可以通过管道连接它们的输入/输出/错误，以及获得它们的返回值。 subprocess.run()函数是在python 3.5以后被添加的，具体参数列表参见python文档，下面只解释几个常用的参数描述。 subprocess.run(args, capture_out=False, shell=False, timeout=None, check=False) 作用： 调用子进程，并返回进程状态码，如果一个进程被执行并结束，则返回0. params: args：指令 ，如果shell=False，则使用python解析器解析(等同于subprocess.call(args))，args为命令参数列表，如果shell=True，则使用shell自己解析，则args为命令字符串； capture_out：如果capture_out设为true，stdout和stderr将会被捕获； timeout：见文档； check：如果check设为true，并且进程以非零状态码退出，一个CalledProcessError异常将会被抛出。 2 Multiprocessing - 基于进程的并行在Unix/Linux系统中提供了一个fork()函数，它比较特殊，普通函数调用一次，返回一次，而fork()函数调用一次，返回两次，因为操作系统会自动地把当前系统复制一份（也就是子进程），然后在父进程和子进程中分别返回. 其中，父进程返回子进程号，而子进程永远返回0. I. 上下文和启动方法根据不同的平台，multiprocessing支持三种启动进程的方法. 这些启动方法有： spawn 父进程启动一个新的Python解释器进程，子进程只会继承那些运行进程对象的run()方法所需的资源，相对于使用fork或forserver，使用这个方法启动进程相当慢。可在Unix和Windows上使用，是在windows上的默认设置。 fork 父进程使用os.fork()来产生Python解释器分叉，只存在于Unix（Unix中的默认值）。 forkserver 程序启动并选择 forkserver 启动方法时，将启动服务器进程。可在Unix平台上使用。 启动方法： 可以使用multiprocessing.set_start_methond(&#39;spawn&#39;)或multiprocessing.get_context(&#39;spawn&#39;)启动。 在程序中set_start_method()不应该被多次调用； 或者，可以使用get_context()来获取上下文对象（获取一个按指定启动方法启动的multiprocessing对象）。上下文对象与多处理模块具有相同的API，并允许在统一程序中使用多个启动方法。 II. 进程间通信进程间可以使用Queue或者Pipe来进行通信。 1. Queue用来在多个进程间进行通信Queue用来在多个进程间进行通信，Queue主要使用两个方法，get()和put(). put(obj [, block=True [, timeout=None]])： 将obj放入到队列中； 如果可选参数block=True，(1)并且timeout是None，将会阻塞当前进程，直到有空的缓冲槽；(2)如果timeout是正数，将会在最多阻塞了timeout秒后，如果还是没有可用的缓冲槽时将抛出queue.Full异常。 如果可选参数block=False(此时timeout参数会被忽略掉)，仅当有可用缓冲槽时才放入队列中，否则抛出queue.Full异常。 get([block [, timeout]]): 从队列中取出并返回对象； block和timeout的作用与put类似，详见python文档。 下面是一个代码实例： 123456789101112131415161718192021222324252627282930313233343536373839import multiprocessingimport osfrom multiprocessing import Processimport timeimport randomimport multiprocessingdef write_process(q, tokens): # 需要执行的子进程 print(\"Write Process`s writing, process id is &#123;:d&#125;.\".format(os.getpid())) for token in tokens: q.put(token) print('put &#123;:s&#125; to the queue.'.format(token)) time.sleep(random.random())def read_process(q): print(\"Read Process`s reading, process id is &#123;:d&#125;.\".format(os.getpid())) # while not q.empty(): # token = q.get() # print(\"get &#123;:s&#125; from the queue.\".format(token)) while True: token = q.get() print(\"get &#123;:s&#125; from the queue.\".format(token))if __name__ == \"__main__\": q = multiprocessing.Queue() # 实例化子进程 pw1 = Process(target=write_process, args=(q, ['t1', 't2', 't3'])) pw2 = Process(target=write_process, args=(q, ['t4', 't5'])) pr = Process(target=read_process, args=(q, )) # 开始执行子进程 pw1.start() pw2.start() pr.start() pw1.join() pw2.join() # pr.join() 如果read_process使用 not q.empty()作为判断条件，由于多进程的异步性，会有元素无法从队列中取出 # read_process里是一个死循环，只能强制终止 pr.terminate() 执行结果 12345678910111213Write Process`s writing, process id is 20408.Read Process`s reading, process id is 23268.Write Process`s writing, process id is 19232.put t1 to the queue.put t4 to the queue.get t1 from the queue.get t4 from the queue.put t5 to the queue.get t5 from the queue.put t2 to the queue.get t2 from the queue.put t3 to the queue.get t3 from the queue. 扩展 在最近的项目中碰到了multiprocessing.SimpleQueue，记录一下. class multiprocessing.SimpleQueue - 这是一个简化的Queue类的实现，很像带锁的Pipe. empty(） - 如果队列为空则返回True，否则返回False. get() - 从队列中移除并返回一个对象. put(item) - 将item放入队列. 2. Pipe常用来在两个进程间通信Pipe常用来在两个进程间通信，两个进程分别位于管道的两端. multiprocessing.Pipe([duplex=True]) - 返回一对Connection对象，(conn1, conn2)分别表示管道的两端; - 如果duplex被置为True，那么该管道是双向的，如果为False，那么该管道是单向的，即conn1只能用于接收消息，conn2只能用于发送消息. 管道是将一系列标准输入输出链接起来的进程，其中每一个进程的输出被直接作为下一个进程的输入. 代码实例： 12345678910111213141516from multiprocessing import Pipedef send(pipe): # 要执行的子进程 pipe.send(\"This is a message.\") pipe.close()if __name__ == \"__main__\": (send_pipe, recv_pipe) = Pipe() # 实例化进程 p = Process(target=send, args=(send_pipe, )) # 执行子进程 p.start() print(recv_pipe.recv()) p.join() 执行结果 1This is a message. 管道同时进行发送信息和接收信息（双工通信）： 1234567891011121314151617181920from multiprocessing import Pipedef send(pipe): # 要执行的子进程 pipe.send(\"This is a message.\") # 管道发送信息 pipe.close() # 关闭管道def sender(pipe): pipe.send(&#123;\"name\": \"sender\", \"content\": \"Do you copy?\"&#125;) recv = pipe.recv() print(\"sender got: \", recv)if __name__ == \"__main__\": (send_pipe, recv_pipe) = Pipe() # Pipe()返回一对管道实例 s = Process(target=sender, args=(send_pipe, )) # 实例化sender进程 s.start() recv = recv_pipe.recv() # recv_pipe是一个Pipe管道，可以接收send_pipe发送过来的消息 print(\"recver got: \", recv) # 打印recv_pipe接收的来自send_pipe的消息 recv_pipe.send(\"roger that.\") # recv_pipe同时又能发送消息 s.join() 执行结果 12recver got: &#123;&apos;name&apos;: &apos;sender&apos;, &apos;content&apos;: &apos;Do you copy?&apos;&#125;sender got: roger that. III. 进程之间的同步在进行并发编程时，通常最好尽量避免使用共享状态，使用多个进程时尤其如此。 但是，如果真的需要使用一些共享数据，那么multiprocessing提供了两种方法。 1. 共享内存可以使用Value或Array将数据存储在共享内存映射中， Value和Array在python3.7中只能供multiprocessing.Process使用，而不能被multiprocessing.Pool使用， multiprocessing.Value和multiprocessing.Array是共享ctypes对象 Value(typecode_or_type, args, lock=True) 属性和方法： value： 获取值 get_lock()：获取锁对象 acquire/release：获取锁/释放锁 介绍： 返回一个从共享内存上创建的ctypes对象，可以通过Value的value属性访问这个对象本身。 typecode_or_type: 指明了返回的对象类型， 对象类型如(附录)Table 1所示。 如果lock是True， 将会新建一个递归锁用于同步对于此值的访问操作，如果lock是Lock或者RLock对象，那么传入的锁将会用于同步对这个值的访问操作，如果lock是False，那么对这个对象的访问将没有锁保护，也就是说这个变量不是进程安全的。 对于+=这样的操作会引发独立的读操作和写操作，也就是说这类操作符不具有原子性。所以，如果你想让递增操作具有原子性，这样的方式并不能达到要求： 1couter.value += 1 假设共享对象内部关联的锁是递归锁(默认情况下)，为了让这样的操作具有原子性，那么可以为该操作加锁，代码如下： 12with couter.get_lock(): counter.value += 1 实例 1234567891011121314151617from multiprocessing import Process, Lock, Valueimport timedef func_sum(val): for i in range(10): time.sleep(0.2) val.value += 1if __name__ == \"__main__\": val = Value('i', 0) process_list = [Process(target=func_sum, args=(val, )) for i in range(10)] for process in process_list: process.start() for process in process_list: process.join() print(val.value) # val.value每次得出的结果都不一样，但不等于100 我们通过创建一个共享对象Value，希望经过100次迭代后，其值能够达到100，但实际的输出并不是100，Value的参数中lock默认是True，意味着将会创建一个递归锁对象用于同步访问控制，然而，要想实现真正的同步访问控制，需要实现获取这个锁，经过修改，代码如下： 123456789101112131415161718192021def func_with_lock(val): for i in range(10): time.sleep(0.2) with val.get_lock(): val.value += 1 \"\"\" 也可以这样获得锁： if val.acquire(): val.value += 1 val.release() \"\"\"if __name__ == \"__main__\": val = Value('i', 0) # process_list = [Process(target=func_sum, args=(val, )) for i in range(10)] process_list = [Process(target=func_with_lock, args=(val,)) for i in range(10)] for process in process_list: process.start() for process in process_list: process.join() print(val.value) # 100 Array(typecode_or_type, size_or_initializer, \\, lock=True*) 从共享内存中申请并返回一个具有ctypes类型的数组对象。 typecode_or_type 指明了返回的数组中的元素类型: 它可能是一个 ctypes 类型或者 array 模块中每个类型对应的单字符长度的字符串。 如果 size_or_initializer 是一个整数，那就会当做数组的长度，并且整个数组会初始化为0。否则，如果 size_or_initializer 会被当成一个序列用于初始化数组中的每一个元素，并且会根据元素个数自动判断数组的长度。 也就是说，size_or_initializer 只能是两种情况： 是一个整数； 是一个序列，如list，tuple， range（python的序列类型就这三种）。 2. 服务器进程由 Manager() 返回的管理器对象控制一个服务器进程，该进程保存Python对象并允许其他进程使用代理操作它们。multiprocessing.Pool只能使用Manager()来实现进程同步。 Manager() 返回的管理器支持类型： list 、 dict 、 Namespace 、 Lock 、 RLock 、 Semaphore 、 BoundedSemaphore 、 Condition 、 Event 、 Barrier 、 Queue 、 Value 和 Array 。 使用示例见1.5.5 多进程读写文件 3 启动大量子进程如果要启动大量的子进程，可以用进程池的方式批量创建子进程。 class multiprocessing.Pool([process [, initializer [, initargs [, maxtasksperchild [ , context]]]]]) 一个进程池对象，它控制可以提交作业的工作进程池。支持带有超时和回调的异步结果，以及一个并行的map实现； params: - process: 使用的工作进程数，如果process=None，则使用os.cpu_count()返回的值 funcs： apply(func[, args[, kwds]]) 使用args参数以及kwds命名参数调用func，它会返回结果前阻塞。这种情况下，apply_async()更适合并行化工作，另外func只会在一个进程池中的一个工作进程中执行。 apply_async(func[, args[, kwds[, callback[, error_callback]]]]) apply()方法的变种，返回一个结果对象。 join() 等待工作进程结束，调用join()前必须先调用close()或者terminate() 代码实例： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() # close() - 阻止后续任务提交到进程池，当所有任务执行完成后，工作进程会退出 p.join() # join() - 等待工作进程结束，调用join()前必须先调用close()或者terminate() print('All subprocesses done.') 执行结果 12345678910111213Parent process 26060.Waiting for all subprocesses done...Run task 0 (21004)...Run task 1 (24740)...Run task 2 (13948)...Run task 3 (25100)...Task 2 runs 0.54 seconds.Run task 4 (13948)...Task 0 runs 0.93 seconds.Task 1 runs 0.98 seconds.Task 3 runs 1.79 seconds.Task 4 runs 1.57 seconds.All subprocesses done. 4 Pool和Process的区别 multiprocessing.Pool可以批量启动子进程，而multiprocessing.Process一次只能启动一个进程，当然multiprocessing.Process可以使用列表生成式来创建一个子进程列表； Process可以直接使用Queue来进行进程间的通信，而Pool如果想实现进程间的通信，只能使用共享内存来实现，即使用Manager().Queue来实现进程间的通信； Process可以直接使用共享内存Array和Value来实现进程同步，而Pool不能直接使用Array和Value，Pool要想实现进程同步、进程通信，可以使用multiprocessing.Manager。 5 多进程读写文件 VS 单进程读写文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091from multiprocessing import Process, Array, RLock, Value, Managerimport mathdef multi_read(multi_val, arr, arr_len, interval, rlock): \"\"\" 多进程分块读数组内容，并返回一个interval长度的字符串 :param multi_arr: :param arr: :param arr_len: :param pid: :param interval: :return: \"\"\" with rlock: start_idx = multi_val.value end_idx = (start_idx + interval) if (start_idx + interval) &lt; arr_len else arr_len multi_val.value = end_idx cur_idx = start_idx string_value = '' while cur_idx &lt; end_idx: string_value += (str(arr[cur_idx]) + '\\n') cur_idx += 1 return string_valuedef write_multiread(s): \"\"\" 向文件中写字符串 该函数是一个回调函数，其接收的参数为 multi_read 返回的值 :param s: :param file_path: :return: \"\"\" file_path = r\"C:\\DevelopWorkspace\\test\\multi_read_multi_write_10_billion.txt\" with open(file_path, 'a+') as file: file.write(s)def multi_read_multi_write(res, multi_read, write_multiread): workers = multiprocessing.cpu_count() p = Pool(workers) manager = Manager() rlock = manager.RLock() start = time.time() logger.info('start time: &#123;&#125;'.format(start)) for sub_list in res: length = len(sub_list) interval = math.ceil(length / workers) multi_val = manager.Value('l', 0) # 异步执行 for i in range(workers): p.apply_async(multi_read, args=(multi_val, sub_list, length, interval, rlock), callback=write_multiread) p.close() p.join() # 单进程读单进程写def single_read_single_write(res): file_path = r\"C:\\DevelopWorkspace\\test\\single_read_single_write_10_billion.txt\" for sub in res: string_value = '' for ele in sub: string_value += str(ele) + '\\n' with open(file_path, 'a+') as file: file.write(string_value) if __name__ == \"__main__\": import time res_sub1 = [i for i in range(1000000)] res_sub2 = [i for i in range(1200000, 3000000)] res = [] res.append(res_sub1) res.append(res_sub2) start = time.time() # multi_read_multi_write 百万级别：2.5175881385803223 # multi_read_multi_write(res, multi_read, write_multiread) # single read and multi write # single_read_multi_write(res, return_value, callback_write) # single read and single write 百万级别：14.876289367675781 single_read_single_write(res) end = time.time() print(\"total time: &#123;&#125;\".format(end - start)) print(\"finished\") 在上述代码的输出中可以看出，百万级别的对比实验中，在8核处理下，多进程读写耗时2.5，单进程耗时14.8，差距还是很明显的。下面提供一个多进程加锁读取一个文件的思路(按块读取，并使用共享内存标记读取位置)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from multiprocessing import Array, Value, RLock, Processimport osimport timeimport multiprocessingimport mathdef multi_read(arr, pid, file_path, FILESIZE, BLOCKSIZE): \"\"\" 对一个文件按照 BLOCKSIZE 字节数大小来读取文件， 通过共享内存multiprocessing.Array来标记读取位置，告诉其他进程该从哪个位置开始读取 :param arr: 共享内存 multiprocessing.Array :param pid: 进程编号 :param file_path: 文件位置 :param FILESIZE: 文件大小 :param BLOCKSIZE: 块大小（该块包含的字节数） :return: \"\"\" print(\"*FILESIZE* \", FILESIZE) with arr.get_lock(): # 通过加锁来修改共享内存，此时对arr的修改是原子操作 start_position = max(arr) end_position = (start_position + BLOCKSIZE) if (start_position + BLOCKSIZE) &lt; FILESIZE else FILESIZE arr[pid] = end_position # 修改共享内存，标记读取位置 fstream = open(file_path, 'r') fstream.seek(start_position) cur_pos = fstream.tell() # 按照块的大小读取文件 while cur_pos &lt; end_position: line = fstream.readline() cur_pos = fstream.tell() # 关闭文件流 fstream.close() print(\"&#123;:d&#125; starts from &#123;:d&#125;, ends to &#123;:d&#125;\".format(pid, start_position, end_position))def get_filesize(file_path): \"\"\" 获取文件大小 -- 文件字节数 :param file_path: :return: \"\"\" fstream = open(file_path, 'r') fstream.seek(0, os.SEEK_END) FILESIZE = fstream.tell() fstream.close() return FILESIZEdef main(): file_path = r'C:\\DevelopWorkspace\\test\\multi_read_multi_write.txt' WORKERS = multiprocessing.cpu_count() FILESIZE = get_filesize(file_path) BLOCKSIZE = math.ceil(FILESIZE / WORKERS) rlock = RLock() arr = Array('l', WORKERS, lock=rlock) print(\"FILE SIZE: &#123;&#125;字节\".format(FILESIZE)) process = [] for i in range(WORKERS): p = Process(target=multi_read, args=(arr, i, file_path, FILESIZE, BLOCKSIZE)) process.append(p) print(\"***MULTIPLE***\") start = time.time() for p in process: p.start() for p in process: p.join() end = time.time() print('total time: &#123;&#125;'.format(end - start)) print(\"***SINGLE**\") s = time.time() single_read(file_path, FILESIZE) e = time.time() print(\"total time: &#123;&#125;\".format(e-s))def single_read(file_path, FILESIZE): fstream = open(file_path, 'r') fstream.seek(0) cur_pos = fstream.tell() while cur_pos &lt; FILESIZE: line = fstream.readline().strip() cur_pos = fstream.tell() fstream.close()if __name__ == \"__main__\": main() # 执行结果FILE SIZE : 24088890字节***MULTIPLE***3 starts from 12044448, ends to 150555607 starts from 18066672, ends to 210777846 starts from 9033336, ends to 120444485 starts from 21077784, ends to 240888904 starts from 15055560, ends to 180666721 starts from 6022224, ends to 90333360 starts from 3011112, ends to 60222242 starts from 0, ends to 3011112total time: 20.560996770858765***SINGLE**total time: 72.99805307388306 经上也可以看出多进程读取文件的速度相对于单进程而言也是占优的。 tips使用进程池，当进程运行完后，不加pool.close()，直接加pool.join()是会报错的，因为进程池里面的进程用完之后不会结束，而是被还到进程池了，因此这里的join检测的是没有任务再进入进程池了，而不是检测子进程的结束。所以要保证没有任务进入进程池，进程池就不会接收到任务，所以pool.close()的作用就是结束进程池接收任务，就是我的任务执行完毕，且没有新的任务进来，这时就被join检测到了。 Appendix对于共享整数或者单个字符，初始化比较简单，参照下表(Table 1)映射关系： Type Code Python Type C Type ‘c’ character char ‘b’ int signed char ‘B’ int unsigned char ‘u’ unicode character Py_UNICODE ‘h’ int signed short ‘H’ int unsigned short ‘i’ int signed int ‘I’ int unsigned int ‘l’ int signed long ‘L’ int unsigned long ‘f’ float float ‘d’ float double 比如共享整数1， 可以使用Value(&#39;h&#39;, 1) 12345&gt; from multiprocessing import Value&gt; &gt; v = Value('h', 1)&gt; print(v.value) # 1&gt; 如果共享对象是一个字符串，则需要使用原始的ctype类型，对应关系如下： ctypes类型 Python类型 C类型 c_bool bool(1) _Bool c_char 单字符字节对象 char c_wchar 单字符字符串 wchar_t c_byte 整型 char c_ubyte 整型 unsigned char c_short 整型 short c_ushort 整型 unsigned short c_int 整型 int c_uint 整型 unsigned int c_long 整型 long c_ulong 整型 unsigned long c_longlong 整型 __int64或 long long c_ulonglong 整型 unsigned __int 64 或 unsigned long long c_size_t 整型 size_t c_ssize_t 整型 ssize_t 或 Py_ssize_t c_float 浮点数 float c_double 浮点数 double c_longdouble 浮点数 long double c_char_p string 或None char * (NUL terminated) c_wchar_p unicode 或None wchar_t * (NUL terminated) c_void_p int 或 None void * 123456&gt; # 上面的Value('h', 1)可以做如下改写&gt; v = Value(c_short, 1)&gt; &gt; # 共享字符串&gt; v = Value(c_char_p, 'hello')&gt;","categories":[{"name":"Python基础","slug":"Python基础","permalink":"http://yoursite.com/categories/Python基础/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"多进程","slug":"多进程","permalink":"http://yoursite.com/tags/多进程/"},{"name":"multiprocessing","slug":"multiprocessing","permalink":"http://yoursite.com/tags/multiprocessing/"},{"name":"Pool","slug":"Pool","permalink":"http://yoursite.com/tags/Pool/"},{"name":"Process","slug":"Process","permalink":"http://yoursite.com/tags/Process/"},{"name":"进程通信","slug":"进程通信","permalink":"http://yoursite.com/tags/进程通信/"},{"name":"进程同步","slug":"进程同步","permalink":"http://yoursite.com/tags/进程同步/"},{"name":"多进程读写","slug":"多进程读写","permalink":"http://yoursite.com/tags/多进程读写/"}]},{"title":"TF-IDF学习","slug":"TF-IDF学习","date":"2020-02-28T08:52:52.000Z","updated":"2020-02-28T14:08:06.965Z","comments":true,"path":"2020/02/28/TF-IDF学习/","link":"","permalink":"http://yoursite.com/2020/02/28/TF-IDF学习/","excerpt":"","text":"TF-IDF学习1. 介绍TF-IDF(term frequency - inverse document frequence)是一种用于信息检索和数据挖掘的常用加权技术。使用TF-IDF是因为计算机只能识别数字，对于文本总出现的单词，计算机无法对它们进行数据处理，需要将文本转换成数学上的概念，便于进一步的训练。 思想： 字词的重要性与该字词在文件中出现的次数成正比，而与该字词在语料库中出现的次数成反比。也就是说，如果一个单词在某篇文章中出现的频率较高，而在其他文章中出现的频率不是很高，则认为该词语具有很好的区分能力，适合用来做分类。 (1) TF是指单词在一个文本中出现的频率，即某个单词的频次/文档中所有单词的频次总和： TF_{i} = {n_{i}\\over \\sum_{j=0}^kn_j}(2) IDF是指逆文档频率，即某个词语的IDF可以由语料库中的文档总数除以包含该单词的文档数，然后取对数得到，它的计算方式如下（加一是为了避免分母为0的情况）： IDF_i = log({文档总数 \\over {包含单词i的文档数 + 1}})(3) TF-IDF就是TF * IDF​ TF-IDF较为简单，没有考虑语义信息，因此不能处理一词多义和一义多词的情形。 (4) 计算过程介绍一个文本经过TF-IDF处理以后，就会变成文本向量，具体如下： 原始文本 1234&gt; text = '人来又人往，玫瑰色花香。已悄悄成为过往。'&gt; # 分词文本&gt; text = ['人 来 又 人往 ， 玫瑰色 花香 。', '已 悄悄 成为 过往 。']&gt; &gt; 获取词袋 text中有两句话，可以将每一句话都看作一个文本，我们可以获取所有文本中的单词(即获取词袋中的单词)，词袋中的单词如下(ordered)， 词袋中单词的数目为6个： 12&gt; ['人往', '悄悄', '成为', '玫瑰色', '花香', '过往']&gt; &gt; 构建文本向量 每一个文本的文本向量的维度是(1, 词袋长度)，那么对于text中的第一句话而言，它的分词结果如下（去除停用词后）： 12&gt; '人 人往 玫瑰色 花香'&gt; &gt; 那么在第一句话的文本向量中的每个元素就是，词袋中每个单词出现在第一句话中的频次。 频次 1 0 0 1 1 0 单词 人往 悄悄 成为 玫瑰色 花香 过往 计算TF-IDF值 这样就为每个文本（每句话）得到了一个向量，以第一句话为例，向量如下： | TF-IDF | 0.58 | 0. | 0. | 0.58 | 0.58 | 0. || ——— | —— | —— | —— | ——— | —— | —— || 单词 | 人往 | 悄悄 | 成为 | 玫瑰色 | 花香 | 过往 | 2. 应用在搜索引擎，关键词提取，文本相似性和文本分类中都有使用。 3. python用sklearn实现中文文本的TF-IDF计算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 使用正则表达式实现文本分句def sent_segmentor(para, chn=True): import re if chn: para = re.sub(r'([。！？\\?])([^”’])', r'\\1\\n\\2', para) para = re.sub(r'(\\.&#123;6&#125;)([^”’])', r'\\1\\n\\2', para) # 匹配出英文省略号 para = re.sub(r'(\\…&#123;2&#125;)([^”’])', r'\\1\\n\\2', para) # 匹配中文省略号，并分行 para = re.sub(r'([。！？\\?])([”’])', r'\\1\\n\\2', para) para = para.rstrip() return re.split(r'\\n', para) else: para = re.sub(r'([\\.\\!\\?])([^\\\"\\'])', r'\\1\\n\\2', para) para = re.sub(r'(\\.&#123;6&#125;)([^\\\"\\'])', r'\\1\\n\\2', para) para = re.sub(r'([\\.\\!\\?])([\\\"\\'])', r'\\1\\n\\2', para) para = para.rstrip() return para.split(r'\\n') # 导入停用词表def stopwords(url): with open(url, 'r', encoding='utf-8') as file: stopwords = file.readlines() stopwords = [line.strip() for line in stopwords] return stopwords# 导入sklearn需要的包from sklearn import feature_extractionfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerif __name__ == \"__main__\": # 导入pkuseg包，用于后面分词 import pkuseg text = '人来又人往，玫瑰色花香。已悄悄成为过往。' stopwords = stopwords(r\"D:\\nlp_tools\\stopwords\\HIT_stopwords.txt\") # 加载分词模型 seg = pkuseg.pkuseg() # 构建语料库 sents = [] for line in sent_segmentor(text): words = [] for word in seg.cut(line): if word not in stopwords: words.append(word) words = \" \".join(words) sents.append(words.strip()) vectorizer = CountVectorizer() tf_idf = TfidfTransformer() matrix = vectorizer.fit_transform(sents) print(\"这是词袋：\") print(vectorizer.get_feature_names()) print(\"这是词频矩阵：\") print(matrix.toarray()) word_tf_idf = tf_idf.fit_transform(matrix) print(\"这是tf-idf矩阵：\") print(word_tf_idf.toarray()) 输出： 12345678910这是词袋：['人往', '悄悄', '成为', '玫瑰色', '花香', '过往']这是词频矩阵：[[1 0 0 1 1 0] [0 1 1 0 0 1]]这是tf-idf矩阵：[[0.57735027 0. 0. 0.57735027 0.57735027 0. ] [0. 0.57735027 0.57735027 0. 0. 0.57735027]] 4. 不足TF-IDF 采用文本逆频率 IDF 对 TF 值加权取权值大的作为关键词，但 IDF 的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以 TF-IDF 算法的精度并不是很高，尤其是当文本集已经分类的情况下。 在本质上 IDF 是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF 的简单结构并不能使提取的关键词， 十分有效地反映单词的重要程度和特征词的分布情 况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被盖。 TF-IDF算法实现简单快速，但是仍有许多不足之处：（1）没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 （2）按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 （3）传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。 （4）对于文档中出现次数较少的重要人名、地名信息提取效果不佳。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[]},{"title":"Keyword Extraction by TextRank","slug":"Keyword-Extraction-by-TextRank","date":"2019-08-01T02:58:12.000Z","updated":"2019-11-22T08:03:28.852Z","comments":true,"path":"2019/08/01/Keyword-Extraction-by-TextRank/","link":"","permalink":"http://yoursite.com/2019/08/01/Keyword-Extraction-by-TextRank/","excerpt":"","text":"Keyword Extraction by TextRank1. Understand PageRankTextRank是一种基于PageRank的算法，通常用来进行关键词抽取和文本摘要生成。PageRank是一个用来计算网页权值的算法，我们可以把网络上的所有网页认为是一个巨大的有向图，一些网页指向另外一些网页，而在图中，一个结点就对应一个网页。如果网页A有通向网页B的连接，则这个连接就可以被认作一个从A到B的有向边。 当我们构建完一整张图以后，我么可以为网页分配权值，使用以下公式： S(V_i) = (1-d) + d * \\sum_{j\\in In(V_i)} { {1}\\over{|Out(V_j)|} }S(V_j) $S(V_i)$ - the weight of webpage i $d$ - damping factor, in case of no outgoing links $In(V_i)$ - inbound links of i, which is a set $Out(V_j)$ - outgoing links of j, which is a set $|Out(V_j)|$ - the number of outbound links 下面用一个例子来讲述PageRank的实现过程。 我们可以用上面的有向图来表示网页之间的连接关系，下图中表示A有一个结点指向E，B有两个结点指向E，我们可以使用邻接矩阵来表示这个有向图。 每一行中的顶点对应的值表示从其他顶点连入该顶点的连接数，即该顶点的入度，同样地，每一列中的顶点对应的值也即该顶点的出度——即该顶点连接出去的数目。 A B E F A 0 0 0 0 B 0 0 0 0 E 1 1 0 0 F 0 1 0 0 根据公式${1 \\over {Out(V_j)}}$，我们应对上述的矩阵作标准化处理，这一步对矩阵所做的处理，实际上也就是将矩阵转换成马尔可夫的转移概率矩阵，通过Markov Chains(实际上网页之间的连接就是一个Markov Chains)的性质来得出最终收敛结果(即该Markov Chains的平稳分布)，也就是我们需要的PageRank。 A B E F A 0 0 0 0 B 0 0 0 0 E 1 0.5 0 0 F 0 0.5 0 0 然后，我们令所有节点的默认权值为1，完成$\\sum_{j\\in In(V_i){1\\over|Out(V_j)|}S(V_j)}$ 的运算，具体如下： \\left[ \\begin{matrix} 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0\\\\ 1 & 0.5 & 0 & 0\\\\ 0 & 0.5 & 0 & 0 \\end{matrix} \\right]* \\left[ \\begin{matrix} 1\\\\ 1\\\\ 1\\\\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} 0\\\\ 0\\\\ 1.5\\\\ 0.5 \\end{matrix} \\right]最后，经过多次迭代直到收敛，就可以得出最后的权值。因为我们预先并不知道node的最终权值，因此，具体的收敛实现则可以用相近两次的权值总和(sum(pagerank))之差来表示，如果这个差小于0.00001，就认为收敛了，即权值就是最终权值。 2.使用TextRank实现Keyword ExtractionPageRank和TextRank的基本思想是一致的，在PageRank中，node是网页，而在TextRank中，node则是单词。在关键词抽取中，我们可以把一个文本分割成许多句子，然后我们存储这些句子中特定词性的词，因为句子中的大多数单词对决定句子的重要性并不是那么有用，我们只考虑名词(NOUN)，专有名词(PROPN)，动词(VERB)等词性，当然这是可以选择的，你也可以使用所有的单词。 例如，我们使用这样一段文本， 12345678910111213141516171819202122232425# e.g.1 将整段文本分割成几个句子 import spacy# 导入en_core_web_sm模型nlp = spacy.load('en_core_web_sm')# 实验文本content = '''The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.'''# 将文本转换为一个nlp对象doc = nlp(content)# 打印文本中的句子for sents in doc.sents: print(sents.text) [out]:# 实验文本被分成了三句话The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics.At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking.While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse. 如果要进行关键词的抽取，那么主要包括以下几个内容： 文本分割成句子，保存每一句中特定词性的单词(表征词 - tokens) 以每一句为单位，获得该句子中指定窗口大小的pair，最终得到整个文本的token_pairs集合 获得所有的tokens的集合，即该文本的表征词汇集合 - vocab 根据获得的vocab和token_pairs，来得到文本图的矩阵表示，并将矩阵归一化 迭代计算出每个token的weight (可选操作)向STOP_WORDS中加入自定义的STOP_WORDS 对权值进行排序 pre-option12345678910import spacyfrom spacy.lang.en.stop_words import STOP_WORDSimport numpy as npfrom collections import OrderedDictnlp = spacy.load(\"en_core_web_sm\")content = '''The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.'''doc = nlp(content) 1. 文本分割成句子，保存每一句中特定词性的单词1234567891011121314151617181920# sentence_segmentcandidate_pos = ['NOUN', 'PROPN']sentences = []for sent in doc.sents: selected_word = [] for word in sent: if word.pos_ in candidate_pos and word.is_stop is False: selected_word.append(word) sentences.append(selected_word)for sent in sentences: print(sent, \"\\n\") [out][Wandering, Earth, China, ’s, budget, science, fiction, thriller, screens, AMC, theaters, North, America, weekend, filmmaking, —, spectacles, China, ’s, epics] [time, Wandering, Earth, throwback, eras, filmmaking] [film, ’s, cast, tone, Chinese, science, fiction, fans, lot, screen, movies] 2. 以每一句为单位，获得该句子中指定窗口大小的pair，最终得到整个文本的token_pairs集合12345678910111213# get_token_pairstoken_pairs = []window_size = 4for sent in sentences: for i, word in enumerate(sent): for j in (i+1, i+window_size): if j &gt;= len(sent): break pair = (word, sent[j]) if pair not in token_pairs: token_pairs.append(pair)print(token_pairs) ​ [output] 1[(Wandering, Earth), (Wandering, budget), (Earth, China), (Earth, science), (China, ’s), (China, fiction), (’s, budget), (’s, thriller), (budget, science), (budget, screens), (science, fiction), (science, AMC), (fiction, thriller), (fiction, theaters), (thriller, screens), (thriller, North), (screens, AMC), (screens, America), (AMC, theaters), (AMC, weekend), (theaters, North), (theaters, filmmaking), (North, America), (North, —), (America, weekend), (America, spectacles), (weekend, filmmaking), (weekend, China), (filmmaking, —), (filmmaking, ’s), (—, spectacles), (—, epics), (spectacles, China), (China, ’s), (’s, epics), (time, Wandering), (time, eras), (Wandering, Earth), (Wandering, filmmaking), (Earth, throwback), (throwback, eras), (eras, filmmaking), (film, ’s), (film, Chinese), (’s, cast), (’s, science), (cast, tone), (cast, fiction), (tone, Chinese), (tone, fans), (Chinese, science), (Chinese, lot), (science, fiction), (science, screen), (fiction, fans), (fiction, movies), (fans, lot), (lot, screen), (screen, movies)] 3. 获得所有的tokens的集合，即该文本的表征词汇集合 - vocab1234567891011# get_vocabvocab = OrderedDict()i = 0for sent in sentences: for word in sent: if word not in vocab: vocab[word] = i i += 1print(vocab)len(vocab) ​ [output] 1OrderedDict([(Wandering, 0), (Earth, 1), (China, 2), (’s, 3), (budget, 4), (science, 5), (fiction, 6), (thriller, 7), (screens, 8), (AMC, 9), (theaters, 10), (North, 11), (America, 12), (weekend, 13), (filmmaking, 14), (—, 15), (spectacles, 16), (China, 17), (’s, 18), (epics, 19), (time, 20), (Wandering, 21), (Earth, 22), (throwback, 23), (eras, 24), (filmmaking, 25), (film, 26), (’s, 27), (cast, 28), (tone, 29), (Chinese, 30), (science, 31), (fiction, 32), (fans, 33), (lot, 34), (screen, 35), (movies, 36)]) 4. 根据获得的vocab和token_pairs，来得到文本图的矩阵表示，并将矩阵归一化12345678910111213141516# get_matrixvocab_size = len(vocab)matrix_for_text = np.zeros((vocab_size, vocab_size), dtype='float')for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] matrix_for_text[i][j] = 1# 在关键词抽取中，我们把文本看做了一个无向图，无向图的矩阵表示是一个对称矩阵，以下就是无向图矩阵的对称化matrix_for_text = matrix_for_text + matrix_for_text.T - np.diag(matrix_for_text.diagonal())# 对矩阵的每一列进行求和 -- 也就是该列顶点对应的出度总和norm = np.sum(matrix_for_text, axis=0)# normalization以后的矩阵matrix_for_text = np.divide(matrix_for_text, norm, where=(norm!=0)) 5. 迭代计算出每个token的weight1234567891011121314151617# analyze()min_diff = 1e-5d = 0.85iterations = 10tr = np.array([1] * vocab_size)print(tr)previous_tr = 0for i in range(iterations): tr = 1-d + d*(np.dot(matrix_for_text, tr)) if abs(previous_tr - sum(tr)) &lt; min_diff: break else: previous_tr = sum(tr)print(tr) ​ [output] 1234567[0.72611111 0.91529514 0.92414931 0.91352431 1.10772569 1.16616319 1.08086806 1.04574653 1.0903125 1.03010417 1.03010417 1.0903125 1.04574653 1.08086806 1.16616319 1.10772569 0.91352431 0.92414931 0.91529514 0.72611111 0.9575 1.12395833 0.91854167 0.91854167 1.12395833 0.9575 0.74116319 0.93034722 1.00442708 0.96428819 1.19361111 1.33232639 1.19361111 0.96428819 1.00442708 0.93034722 0.74116319] 6. 对权值进行排序123456789# get_keywords()node_weight = dict()for word, index in vocab.items(): node_weight[word] = tr[index]node_weight_ordered = OrderedDict(sorted(node_weight.items(), key=lambda t:t[1], reverse=True))for k, v in node_weight_ordered.items(): print(k, \" -- \", v) ​ [output] 12345678910111213141516171819202122232425262728293031323334353637science -- 1.3323263888888888Chinese -- 1.1936111111111112fiction -- 1.1936111111111112science -- 1.1661631944444442filmmaking -- 1.1661631944444442Wandering -- 1.1239583333333334eras -- 1.1239583333333334budget -- 1.1077256944444445— -- 1.1077256944444445screens -- 1.0903125North -- 1.0903125fiction -- 1.0808680555555554weekend -- 1.0808680555555554thriller -- 1.0457465277777778America -- 1.0457465277777778AMC -- 1.0301041666666666theaters -- 1.0301041666666666cast -- 1.004427083333333lot -- 1.004427083333333tone -- 0.9642881944444444fans -- 0.9642881944444444time -- 0.9574999999999999filmmaking -- 0.9574999999999999screen -- 0.9303472222222222’s -- 0.9303472222222221China -- 0.9241493055555555China -- 0.9241493055555555Earth -- 0.9185416666666666throwback -- 0.9185416666666666Earth -- 0.9152951388888888’s -- 0.9152951388888888’s -- 0.9135243055555555spectacles -- 0.9135243055555555film -- 0.7411631944444443movies -- 0.7411631944444443Wandering -- 0.726111111111111epics -- 0.726111111111111 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import numpy as npimport spacyfrom collections import OrderedDictfrom spacy.lang.en.stop_words import STOP_WORDSnlp = spacy.load(\"en_core_web_sm\")class KeywordExtraction(): def __init__(self): self.d = 0.85 # damping factor self.min_diff = 1e-5 # self.steps = 10 # iterate steps self.node_weight = None # node_weight def set_stopwords(self, stopwords): ''' 添加自定义 stop_words :param stopwords: :return: ''' for word in STOP_WORDS.union(set(stopwords)): lexeme = nlp.vocab[word] lexeme.is_stop = True def sentence_segment(self, doc, candidate_pos, lower=False): ''' 保存每一句中特定词性的单词 :return: ''' sentences = [] for sent in doc.sents: selected_words = [] for word in sent: if word.pos_ in candidate_pos and word.is_stop is False: if lower: selected_words.append(word.text.lower()) else: selected_words.append(word.text) sentences.append(selected_words) return sentences def get_vocab(self, sentences): ''' 得到所有句子中特定词性的单词 :param sentences: :return: ''' vocab = OrderedDict() i = 0 for sent in sentences: for word in sent: if word not in vocab: vocab[word] = i i += 1 return vocab def get_token_pairs(self, window_size, sentences): ''' :param window_size: :param sentences: :return: ''' token_pairs = [] for sent in sentences: for i, word in enumerate(sent): for j in range(i+1, i+window_size): if j &gt;= len(sent): break pair = (word, sent[j]) if pair not in token_pairs: token_pairs.append(pair) return token_pairs def symmetrize(self, matrix): return matrix + matrix.T - np.diag(matrix.diagonal()) def get_matrix(self, vocab, token_pairs): vocab_size = len(vocab) g = np.zeros((vocab_size, vocab_size), dtype='float') for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] g[i][j] = 1 g = self.symmetrize(g) sum = np.sum(g, axis=0) ''' np.divide(x1, x2, where=True) x1: array_like 被除矩阵 x2: array_like 除数矩阵 如果x1和x2的shape不一致的话，他们必须符合数组广播规则，也就是输出矩阵的shape -- 换言之，输入数组必须具有相同的形状或符合数组广播规则 ''' g_after_divide = np.divide(g, sum, where=sum!=0) return g_after_divide def get_keywords(self, num=10): node_weight = OrderedDict(sorted(self.node_weight.items(), key= lambda t: t[1], reverse=True)) for i, (k, v) in enumerate(node_weight.items()): print(k, \" -- \", v) if i &gt; num: break def analyze(self, doc, window_size=4, candidate_pos=['NOUN', 'PROPN'], lower=False, stopwords=list()): self.set_stopwords(stopwords) doc_sentences = self.sentence_segment(doc, candidate_pos, lower) doc_vocab = self.get_vocab(doc_sentences) doc_token_pairs = self.get_token_pairs(window_size, doc_sentences) doc_matrix = self.get_matrix(doc_vocab, doc_token_pairs) vocab_size = len(doc_vocab) tr = np.array([1] * vocab_size) previous_tr = 0 for i in range(self.steps): tr = 1-self.d + self.d * np.dot(doc_matrix, tr) if abs(previous_tr - sum(tr)) &lt; self.min_diff: break previous_tr = sum(tr) node_weight = dict() for word, index in doc_vocab.items(): node_weight[word] = tr[index] self.node_weight = node_weightif __name__ == '__main__': textrank = KeywordExtraction() text = ''' The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse. ''' doc = nlp(text) textrank.analyze(doc, 4) # 取前10个权值最高的单词 textrank.get_keywords(10)","categories":[{"name":"TextRank","slug":"TextRank","permalink":"http://yoursite.com/categories/TextRank/"}],"tags":[]},{"title":"Python正则表达式-不完全使用指南","slug":"Python正则表达式-不完全使用指南","date":"2019-07-22T12:39:52.000Z","updated":"2019-08-01T03:23:01.027Z","comments":true,"path":"2019/07/22/Python正则表达式-不完全使用指南/","link":"","permalink":"http://yoursite.com/2019/07/22/Python正则表达式-不完全使用指南/","excerpt":"","text":"1. 基础字符1.1 元字符元字符是一些特殊的metacharactes， 并且不匹配自己。相反，他们表示应该匹配一些与众不同的东西，或者通过重复他们或改变他们的含义来影响正则的其他部分。元字符如下： . ^ $ * + ? { } [ ] \\ | ( ) 在元字符”[ ]”中，可以单独列出字符，也可以通过给出两个字符并用”-“标记将它们分开来表示一系列字符。例如，[abc]将匹配任何字符a、b或c（匹配单个字符），这与[a-c]相同，它使用一个范围来表示同一组字符。 字符类中的元字符不生效。例如，[akm$]将匹配’a’, ‘k’, ‘m’, ‘$’中的任意字符; ‘$’通常是一个元字符，但在一个字符类中它被剥夺了特殊性。 1.2 预定义字符集 \\d 匹配任何十进制数字；这等价于[0-9] \\D 匹配任何非数字字符；这等价于[^0-9] \\s 匹配任何空白字符；这等价于[ \\t\\n\\r\\f\\v] \\S 匹配任何非空白字符；这等价于[^ \\t\\n\\r\\f\\v] \\w 匹配任何字母与数字字符；这相当于类[a-zA-Z0-9] \\W 匹配任何非字母与数字字符；这相当于类[^a-zA-Z0-9] 1.3 常用操作符 操作符 说明 实例 . 表示任意单个字符 对于大小写字母、各种符号都能匹配 [ ] 字符集，对单个字符给出取值范围 [abc]表示a、b、c,[a-z]表示a到z的单个字符 非字符集，对单个字符给出排除范围 abc 表示非a或b或c的单个字符 * 前一个字符0次或无限次扩展 abc*表示ab、abc、abcc、abccc等 + 前一个字符1次或无限次扩展 abc+表示abc、abcc、abccc等 ？ 前一个字符0次或1次扩展 abc？表示ab、abc \\ 左右表达式任意一个 abc\\ def表示abc、def {m} 扩展前一个字符m次 ab{2}c表示abbc {m, n} 扩展前一个字符m至n次（包 含n） ab{1, 2}表示ab、abb ^ 匹配字符串开头 ^abc表示abc且在一个字符串的开头 $ 匹配字符串结尾 abc$表示abc且在一个字符串的结尾 ( ) 分组标记，内部只能使用 \\ 操作符 (abc)表示abc，(abc \\ def)表示abc、def \\d 表示十进制数字 \\w 匹配任何字母与数字字符 相当于[a-zA-Z0-9] tips: ?字符可以看作是一个可选字符，如home-?grew，其表示home-grew或者homegrew，即当作 ? 的前一个字符是可选的的； 对于{m, n}来说，如果不限定m和n的数值，即{ , }，则认为下限m的值为0，而上限n的值则为无穷，因此： *也就等价于{0, } +也就等价于{1, } ?也就等价于{0, 1} 但在实际中，还是建议使用*、+、?的形式，这样更短更容易阅读 2. 使用正则表达式有了前面的基础字符知识，我们已经可以使用正则表达式了，在python中如何使用它们呢？re模块提供了正则表达式引擎的接口，允许你将正则编译为对象，然后用它们进行匹配。 2.1 编译正则表达式正则表达式首先要编译成模式对象，然后才能进行各种操作。 12345678import rere_pattern = re.compile('ab*')print(\"re_pattern : \",re_pattern)print(\"type :\", type(re_pattern))[output]re_pattern : re.compile('ab*')type : &lt;class 're.Pattern'&gt; 需要明确的是，正则是作为字符串传递给re.compile()的，正则被处理为字符串，因为正则表达式不是核心python语言的一部分，并且没有创建用于表达它们的特殊语法。 可以理解的是，在正则被re模块编译之前，它就是一个普通的字符串，这与python语言所理解的字符串并无二致，如前所述，正则表达式使用反斜杠字符 \\ 来表示特殊形式或允许使用特殊字符而不调用它们的特殊含义，这就会与python字符串中的反斜杠用途发生冲突，导致反斜杠灾难。 例如我们要编写一个与 \\expression相匹配的正则，那么编译之后得到的目标模式就应该是 \\ \\expression，在编译之前，正则就是一个普通的字符串，对于每一个 \\，在字符串中表示就是 \\ \\，也就是说，编译前的字符串就应该是 \\ \\ \\ \\ expression，可见，对目标字符串中如果有一个反斜杠，那么传给编译器的正则表达式中就会有4个反斜杠，十分繁琐。 字符 阶段 \\expression 被匹配的字符串 \\ \\ expression 为 re.compile()转义的反斜杠 “\\\\\\\\expression” 为字符串字面转义的反斜杠 解决方案是使用python的原始字符串表示法来表示正则表达式；反斜杠不以任何特殊的方式处理前缀为 ‘r’的字符串字面，因此 r’\\n’ 是一个包含 ‘\\’和 ‘n’的双字符字符串，而 ‘\\n’是一个包含换行符的单字符字符串。正则表达式通常使用这种原始字符串表示法用python代码编写。 常规字符串 原始字符串 “ab*” r”ab*” “\\\\\\\\expression” r”\\\\expression” “\\\\w+\\\\s+\\\\l” r”\\w+\\s+\\l” tips: 1234567891011121314&gt; # 带反斜杠的特殊字符可以用\\d或者\\\\d来匹配&gt; pattern = re.compile(\"\\\\d\")&gt; result = pattern.findall(\"space hh99\")&gt; print(result)&gt; &gt; [output] : ['9', '9']&gt; &gt; # 匹配一个反斜杠&gt; pattern = re.compile(\"\\\\\\\\\")&gt; result = pattern.findall(\"n\\\\b\\\\a\")&gt; print(result)&gt; &gt; [output] : ['\\\\', '\\\\']&gt; &gt; 3. 应用匹配3.1 re库的常用函数 函数 说明 search() 扫描字符串，查找此正则匹配的任何位置，返回match对象 match() 从一个字符串的开始位置其匹配正则表达式，返回match对象 findall() 搜索字符串，以列表类型返回全部能匹配的子串 split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型 finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 3.2 匹配对象 方法/属性 目的 group() 返回正则匹配的字符串 start() 返回匹配的开始位置 end() 返回匹配的结束位置 span() 返回包含匹配(start, end)位置的元组 group()返回正则匹配的子字符串。start()和end()返回匹配的其实和结束索引。span()在单个元组中返回开始和结束索引。由于match()方法只检查正则是否在字符串的开头匹配，所以start()将始终为0。 注意： 只有match对象才有上述属性，也就是说，对一个模式只有使用search()、match()和finditer()后所返回的match对象才有上述属性，而对于使用findall()等方法，由于其返回对象是一个list，所以没有上述属性。 1234567891011121314151617181920212223242526import reresult1 = pattern.match(\"little boy found his favorite toy\")result2 = pattern.match(\"littleboyfoundhisfavoritetoy\")print(result1)print(result2)[output]&gt;&gt;&gt; &lt;re.Match object; span=(0, 6), match='little'&gt;&gt;&gt;&gt; &lt;re.Match object; span=(0, 28), match='littleboyfoundhisfavoritetoy'&gt;result2.group()[output]&gt;&gt;&gt; 'littleboyfoundhisfavoritetoy'result2.start()[output]&gt;&gt;&gt; 0result2.end()[output]&gt;&gt;&gt; 28result2.span()[output]&gt;&gt;&gt; (0, 28) 但是，模式的search()方法会扫描字符串，因此在这种情况下匹配可能不会从0开始。 12345678910111213141516result3 = pattern.search(\"****little boy`s here\")result3.group()[output]&gt;&gt;&gt; 'little'result3.start()[output]&gt;&gt;&gt; 4result3.end()[output]&gt;&gt;&gt; 10result3.span()[output]&gt;&gt;&gt; (4, 10) 4. 更多模式能力4.1 分组通常，你需要获取更多信息，而不仅仅是正则是否匹配。正则表达式通常用于通过将正则分成几个子组来解析字符串，这些子组匹配不同的感兴趣组件。 例如，现有如下信息： From: author@example.com User-Agent: Thunderbird MIME-Version: 1.0 To: editor@example 我们想要提取每一个 ：后面的信息，因此可以使用分组来进行匹配， 1234567891011121314151617181920212223242526272829303132# e.g. 4.1.1info = \"From: author@example.com\\nUser-Agent: Thunderbird\\nMIME-Version: 1.0\\nTo: editor@example\"print(info)[output]&gt;&gt;&gt; From: author@example.com User-Agent: Thunderbird MIME-Version: 1.0 To: editor@examplepattern1 = re.compile(r\"From:\\s+(\\w+.\\w+.\\w+)\\s+User-Agent:\\s+(\\w+)\\s+MIME-Version:\\s+(\\d+.\\d+)\\s+To:\\s+(\\w+.\\w+)\", re.IGNORECASE)result = pattern1.findall(info)print(result)[output]&gt;&gt;&gt;[('author@example.com', 'Thunderbird', '1.0', 'editor@example')]result1 = pattern1.finditer(info)for result in result1: result.group(1) result.group(2) result.group(3) result.group(4) Out[108]: 'author@example.com'Out[108]: 'Thunderbird'Out[108]: '1.0'Out[108]: 'editor@example' 再上述4.1.1的例子中，我们在模式中写了四个分组，分别匹配From、User-Agent、MIME-Version和To后面的内容，这四个分组被自动地编号，分别为1、2、3、4，同时被捕获到了相应的内存中，这就引入了分组的后向引用，例如，我要进行一个双词的匹配，请看下例： 123456789101112#e.g. 4.1.2raw = \"loving loving can hurt\"'''在匹配之前，(\\w+)这个分组可以匹配任何单词内容，一旦当这个分组匹配完毕，在这里，一开始就匹配了loving这个单词，因此，就把loving捕获，放在分组1的内存单元，再经过一个空格后，把分组1中的捕获内容取出，与当前单词比较，看是否匹配，如果匹配，则结束，否则，重新匹配这个分组。'''pattern = re.compile(r\"\\b(\\w+)\\b\\s+\\1\")print(pattern.findall(raw))[output]&gt;&gt;&gt;['loving'] 4.2 忽略某个分组有时候给正则的某个子表达式加括号并不是为了分组，而仅仅是为了看起来更清晰，因此我们并不需要捕获该分组，那么，可以使用(?:expression)来忽略该分组。 123456789101112131415161718192021# e.g. 4.2.1raw = \"age:13, name:Tom\"pattern = re.compile(r\"age.(\\d+).\\s*name.(\\w+)\")print(pattern.findall(raw))[out]: [('13', 'Tom')]print(pattern.search(raw).group(1))[out]: 13 print(pattern.search(raw).group(2))[out]: Tom pattern1 = re.compile(r\"age.(\\d+).\\s*name.(?:\\w+)\")print(pattern1.findall(raw))[out]: ['13']print(pattern1.search(raw).group(1))[out]: 13# 由于忽略了第二个分组，因此无法检索该分组print(pattern1.search(raw).group(2))[out]: IndexError: no such group 需要注意的是，除了你无法检索组匹配内容的事实外，非捕获组的行为与捕获组完全相同；你可以在里面放任何东西，用重复元字符重复它，比如*，燃火把它嵌入其他组（捕获或不捕获）。(?:…..)在修改现有模式的时候特别有用，因为你可以添加新组而不更改所有其他组的编号方式。红色部分不懂什么意思。值得一提的是，捕获和非捕获组之间的搜索没有性能差异，两种形式没有哪一种更快。 4.3 命名分组命名组的语法是python特定的扩展之一：(?P…)。name是该分组的名称。命名组的行为与捕获组完全相同，并且还将名称与组关联。处理捕获组的的匹配对象方法都接受 1. 按编号引用该分组；2. 按组名字符串引用该分组。 12345678910raw = \"sheeran sheeran is good\"pattern = re.compile(r\"(?P&lt;name&gt;\\b\\w+\\b)\\s+(?P=name)\")print(pattern.findall(raw))[out]: ['sheeran']pattern = re.compile(r\"(?P&lt;name&gt;\\b\\w+\\b)\\s+\\1\")print(pattern.findall(raw))[out]: ['sheeran'] 4.4 前向断言另一个零宽度断言是前向断言。前向断言以正面和负面形式提供，如下所示： 正前向断言 — (?=…)如果包含的正则表达式，由...表示，在当前位置成功匹配，则成功，否则失败。但是，一旦尝试了包含的表达式，匹配的引擎就不会前进，模式其余的部分会在断言开始的地方尝试。 12345678910# 选出得分大于30分的球员raw = [\"James:36.33\", \"Bryant:33.2\", \"ONeal:24.33\"]pattern = re.compile(r\".+(?=[:][3][0-9][.][0-9]+)\")for it in raw: print(pattern.findall(it)) [out]: ['James'] ['Bryant'] [] 负前向断言 — (?!…)如果包含的表达式在字符串中的当前位置不匹配，则成功。 123456789101112131415161718192021'''现有一些文件如下：[\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\", \"start.end.bat\"]我们需要从中选取出后缀不是bat的文件，因此可以使用负前向断言来匹配后缀不是bat的文件'''raw = [\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\", \"start.end.bat\"]pattern = re.compile(r\".*[.](?!bat$)[^.]*\")for it in raw: print(pattern.findall(it)) [out]: ['sample.txt'] ['ss.batch'] [] ['computer.sss'] ['name.is.wiki'] ['haha.bat.ten'] [] 在上述代码中，模式是r&quot;.*[.](?!bat$)[^.]*&quot;，意思是，从结尾匹配bat，如果匹配，则该模式失败，如果结尾不匹配bat，那么开始匹配模式[^.]*$。也就是说，如果结尾不匹配bat，我们使用[^.]*$ 就可以把不匹配的后缀部分输出。如果我们使用的模式是r&quot;.*[.](?!bat$)，那么就相当于把后缀不是bat的.*[.] 输出，即只是输出后缀前面的部分: 123456789101112raw = [\"sample.txt\", \"ss.batch\", \"text.bat\", \"computer.sss\", \"name.is.wiki\", \"haha.bat.ten\"]pattern = re.compile(r\".*[.](?!bat$)\")for it in raw: print(pattern.findall(it)) [out]: ['sample.'] ['ss.'] [] ['computer.'] ['name.is.'] ['haha.bat.'] tips: 在平常使用中，更感觉前向断言像是一个筛选条件，我们可以匹配一个字符或字符串，在匹配对象的后面或者前面我们设定一些条件，可以用前向断言来实现。 5. 修改字符串到目前为止，我们只是针对静态字符串执行搜索。正则表达式通常也用于以各种方式修改字符串，使用以下模式方法： 方法/属性 目的 split() 将字符串拆分成一个列表，在正则匹配的任何地方将其拆分 sub() 找到正则匹配的所有子字符串，并用不同的字符串替换它们 subn() 与sub()相同，但返回新字符串和替换次数 5.1 分割字符串模式的split()方法在正则匹配的任何地方拆分字符串，返回一个片段列表。它有两种使用方式： pattern.split(string [, maxsplit = 0]) re.split(pattern, string [, maxsplit = 0]) 通过正则表达式的匹配拆分字符串，如果在正则中使用捕获括号，则它们的内容也将作为结果列表的一部分返回。如果maxsplit非零，则最多执行maxsplit次拆分。 12345678910111213# 分隔符是任意非字母数字字符序列p = re.compile(r\"\\W+\")# 不设定maxsplitresult1 = p.split(\"this is a test, short and sweet, of split()\")print(result1)[out]: ['this', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']# 设定maxsplit = 3，则最多在字符串上分隔3次result2 = p.split(\"this is a test, short and sweet, of split()\", maxsplit = 3)print(result2)[out]: ['this', 'is', 'a', 'test, short and sweet, of split()'] 如果我们不单单想要根据分隔符分隔出的文本，而且还需要知道分隔符是什么，因此，如果在正则中使用捕获括号，则分隔符的内容也会作为列表的一部分返回。 12# 在这一句中，分隔符是“， ”re.split(r\"([\\W]+)\", \"words, words, words\") 5.2 搜索和替换另一个常见任务是找到模式的所有匹配项，并用不同的字符串替换它们。sub()方法接受一个替换值，可以是字符串或函数，也可以是要处理的字符串。 pattern.sub(replacement, string[, count = 0]) 使用replacement来替换string中的匹配对象，如果没有找到模式，则string将保持不变。 可选参数count是要替换的模式最大出现次数，count必须是非负整数。默认为0表示替换所有。 123456789101112131415raw = \"sheeran is a good singer, and I think sheeran is awesome!\"pattern = re.compile(r\"sheeran\")pattern.sub(\"jay\", raw)[out]: 'jay is a good singer, and I think jay is awesome!'pattern.sub(\"Jay\", raw, count = 1)[out]: 'Jay is a good singer, and I think sheeran is awesome!'# subn的用法和sub一致，但返回一个包含新字符串值和一致性的替换次数的2元组。 pattern.subn(\"Jay\", raw)[out]: ('Jay is a good singer, and I think Jay is awesome!', 2) 需要注意的是，如果replacement是一个字符串，则处理其中的任何反斜杠转义。也就是说，\\n被转换为单个换行符，\\r 被转换为回车符，依此类推。 还有一种语法用于引用由(?P&lt;name&gt;...)语法定义的命名组。\\g&lt;name&gt; 将使用名称为name 的组匹配的子字符串，\\g&lt;number&gt; 使用相应的组号(后向引用)，因此\\g&lt;2&gt; 等同于\\2 ，但在诸如\\g&lt;2&gt;0 之类的替换字符串中并不模糊。而\\20 则被解释为对组20捕获的内容惊醒引用，而不是对组2的引用，因此不建议使用\\20 这种用法，以下几种用法都是等效的： 123456789101112131415player = \"James&#123;Forward&#125; Rondo&#123;Guard&#125; Cousins&#123;Center&#125;\"p = re.compile(r\"\\w+&#123;(?P&lt;position&gt;[^&#125;]*)&#125;\")# 把所有匹配正则的子字符串替换成 player(\\g&lt;position&gt;)p.sub(r\"player(\\g&lt;position&gt;)\", player)[out]: 'player(Forward) player(Guard) player(Center)'p.sub(r\"player(\\g&lt;1&gt;)\",player)[out]: 'player(Forward) player(Guard) player(Center)'p.sub(r\"player(\\1)\", player)[out]: 'player(Forward) player(Guard) player(Center)' 此外，replacement也可以是一个函数，它可以为你提供更多控制。如果replacement是一个函数，则为patter的每次非重叠出现将调用该函数。在每次调用时，函数都会传递一个匹配的匹配对象参数，并可以使用此信息计算所需的替换字符串并将其返回。 1234567891011121314# 实现将句中的正数转换为浮点数def transfer(match): value = int(match.group()) after_transfer = float(value) # 字符类型转换 return str(after_transfer)raw = \"James scored 33 points and Davis scored 29 points\"p = re.compile(r\"\\d+\")'''需要注意的时，sub(replacement, string)，replacement一般都是一个字符型对象。'''p.sub(transfer, raw)","categories":[{"name":"Python基础","slug":"Python基础","permalink":"http://yoursite.com/categories/Python基础/"}],"tags":[]},{"title":"leetcode-0019-删除链表的倒数第N个个节点","slug":"leetcode-0019-删除链表的倒数第N个个节点","date":"2019-06-04T06:32:20.000Z","updated":"2019-07-22T12:30:01.334Z","comments":true,"path":"2019/06/04/leetcode-0019-删除链表的倒数第N个个节点/","link":"","permalink":"http://yoursite.com/2019/06/04/leetcode-0019-删除链表的倒数第N个个节点/","excerpt":"","text":"[题目表述]给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。 示例： 给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5.说明： 给定的 n 保证是有效的。 进阶： 你能尝试使用一趟扫描实现吗？ def removeNthFromEnd_one_iter(self, head, n): dummy = ListNode(0) dummy.next = head first = dummy second = dummy while n &gt;= 0: first = first.next n -= 1 while first: second = second.next first = first.next second.next = second.next.next return dummy.next","categories":[{"name":"leetcode刷题","slug":"leetcode刷题","permalink":"http://yoursite.com/categories/leetcode刷题/"}],"tags":[]}]}